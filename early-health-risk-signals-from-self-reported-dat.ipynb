{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Self Collected Health Data ML Pipeline Code: Comprehensive Explanation of the AI/ML Pipeline for Early Health Risk Signal Detection\n\nThis document provides an in depth walkthrough of the provided Python script, which implements a complete machine learning pipeline for detecting early health risk signals from self reported data. It uses only open source datasets (NHANES, UCI, BRFSS, etc.) and does not require medical records. The pipeline is designed with production grade considerations: memory efficiency, anti overfitting measures, ensemble/hybrid modelling, and thorough evaluation. Below we dissect every component, explain the architectural decisions, and finally interpret the execution results.\n________________________________________\n\n1. Overall Architecture & Goals\nThe pipeline aims to build a robust classifier that can flag individuals at elevated health risk based on lifestyle and basic clinical measurements (e.g., age, BMI, blood pressure, smoking status). Key design choices:\n•\tEnsemble / Hybrid Approach – Combines classical ML (GradientBoosting, RandomForest, LogisticRegression, XGBoost, LightGBM) with a neural network that includes a Multi Head Attention mechanism. The final predictions can be an unweighted average or a stacking ensemble.\n•\tFour Way Data Split – To rigorously estimate real world performance, the data is split into Train (40%), Validation (15%), Test (15%), and Holdout (30%). The holdout set is never touched until final evaluation, acting as a proxy for truly unseen data.\n•\tAnti Overfitting Techniques – L2 regularisation, Dropout (40%), Batch Normalisation, EarlyStopping, and sample weighting based on IsolationForest outlier scores.\n•\tMemory Efficient Processing – Chunked operations, dtype optimisation, emergency garbage collection when memory exceeds a threshold (80%), and careful handling of large datasets via downsampling.\n\n________________________________________\n\n2. Detailed Code Walkthrough\n2.1 Imports and Fallbacks\nThe script begins by importing standard libraries (os, sys, json, logging, etc.) and third party packages. It gracefully handles optional dependencies:\n•\tIf tqdm is missing, a minimal substitute is provided.\n•\tIf xgboost or lightgbm are not installed, the corresponding models are skipped with a warning.\n•\tFor TensorFlow, GPU memory growth is configured and memory limits are set to avoid OOM. If TensorFlow is unavailable, the neural network part is skipped.\n2.2 Global Configuration\nDirectories for plots, models, and reports are created. Key parameters are defined:\n•\tRANDOM_STATE = 42 for reproducibility.\n•\tMAX_MEMORY_PERCENT = 80 triggers emergency GC.\n•\tCHUNK_SIZE, MAX_CHUNKS, SAMPLE_FRAC for memory safe processing.\n•\tNeural network hyperparameters (BATCH_SIZE_NN, NN_EPOCHS, NN_PATIENCE).\n•\tThe four way split proportions.\n•\tA colour palette for consistent visualisation.\n2.3 Memory Management Functions\n•\tget_memory_usage() – queries process and system memory via psutil.\n•\tlog_memory() – logs current memory usage with a custom tag.\n•\tforce_cleanup() – aggressively deletes objects and runs gc.collect() three times to free memory.\n•\tcheck_memory_limit() – if system memory exceeds threshold, triggers emergency GC and returns True.\n•\toptimize_dtypes() – downcasts float64 → float32, int64 → smaller int types, and converts low cardinality object columns to category. This can reduce memory footprint significantly.\n2.4 Data Loading (DataLoader class)\n   \nThe loader attempts to retrieve data from multiple open sources in priority order:\n\n1.\tNHANES 2017 2018 – Downloads several .XPT files from the CDC FTP server, merges them on the subject identifier (SEQN), and renames columns to human friendly names (e.g., RIDAGEYR → age). If the merged dataset is large (>8000 rows), it is sampled to SAMPLE_FRAC.\n2.\tUCI Heart Disease – Fetches the processed Cleveland dataset (or a GitHub mirror) and binarises the target (>0 = disease).\n3.\tSleep Health Lifestyle CSV – A Kaggle style dataset on sleep and lifestyle.\n4.\tSynthetic NHANES – If all else fails, a synthetic dataset is generated from published NHANES summary statistics (NCHS Data Brief No. 373). It creates realistic continuous variables (age, BMI, cholesterol, etc.) and derives a binary health_risk label from a linear combination of features.\nThe loader returns a pandas DataFrame and records the data source used.\n2.5 Preprocessing & EDA (Preprocessor class)\nAfter loading, the pipeline runs an Exploratory Data Analysis:\n•\tClass distribution (bar and pie charts).\n•\tMissing value fractions (horizontal bar chart).\n•\tNumerical distributions per class (histograms).\n•\tCorrelation heatmap.\n•\tFeature vs. target boxplots.\nAll plots are saved to ./plots/ with descriptive names.\nThe clean() method performs:\n•\tDropping rows/columns that are entirely empty.\n•\tRemoving duplicate rows.\n•\tDropping columns with >80% missing values.\n•\tSeparating numerical and categorical columns.\n•\tImputing numerical missing values with median, categorical with most frequent.\n•\tLabel encoding categorical variables.\nOutliers are handled via IQR based winsorizing (remove_outliers_iqr): values outside Q1 - k*IQR and Q3 + k*IQR are clipped (not dropped). A bar chart shows outlier counts per feature.\nFinally, the scale() method uses RobustScaler (fitted on training data) to transform all splits – robust to outliers.\n2.6 Feature Engineering (FeatureEngineer class)\nThis class adds domain informed features to improve predictive power:\n•\tCardiovascular risk proxy – linear combination of age, systolic BP, cholesterol, HDL, and smoking.\n•\tMetabolic syndrome score – counts how many of five criteria are met (waist circumference, glucose, BP, HDL, triglycerides proxy).\n•\tLifestyle composite – aggregates smoking, alcohol, physical activity, sleep, stress, depression.\n•\tSleep stress interaction – stress_level / sleep_hours.\n•\tBMI age interaction – bmi * age / 100 and bmi².\n•\tBlood pressure features – pulse pressure, mean arterial pressure, hypertension stage.\n•\tGlucose lipid features – prediabetes/diabetes flags, HbA1c risk category, cholesterol ratio.\n•\tPolynomial interactions – e.g., bmi x age, systolic_bp x age.\nAfter engineering, feature selection is performed using a combination of mutual information and RandomForest importance. The top n_features (default 35) are retained. Optionally, PCA components can be appended (compute_pca), and the explained variance is plotted.\n2.7 Data Splitting (four_way_split function)\nA strict four way stratified split is implemented:\n1.\tSeparate holdout (30%) using StratifiedShuffleSplit.\n2.\tFrom the remaining 70%, carve out test (15% of total, i.e., 15/70 ≈ 21.43% of the development set).\n3.\tFrom the remaining train+val, split into train (40% of total) and val (15% of total) with the appropriate proportions.\nThe function verify_data_splits checks for index leakage (no overlap) and verifies that class proportions are close to the original.\n2.8 Sample Weights via IsolationForest\ncompute_sample_weights fits an IsolationForest on the training data and converts the anomaly scores to weights in the range [0.3, 1.0]. Outliers receive lower weight, reducing their influence during training. This is a robust way to handle atypical samples without discarding them.\n2.9 Neural Network with Attention (build_attention_nn)\nIf TensorFlow is available, a custom model is built:\n•\tInput → GaussianNoise (0.05) for augmentation.\n•\tTwo dense blocks (128, 64) with ReLU, BatchNorm, Dropout (0.4, 0.35), and L2 regularisation.\n•\tA Multi Head Attention layer (4 heads, key dimension 16) operates on a reshaped sequence (1 time step, 64 features). Residual connection + LayerNorm follows.\n•\tGlobal average pooling flattens the attended features.\n•\tFinal dense block (32) and a sigmoid output.\n•\tCompiled with Adam (lr=1e 3), binary crossentropy, and metrics (accuracy, AUC, precision, recall).\nCallbacks: EarlyStopping (patience=10), ModelCheckpoint, ReduceLROnPlateau.\n2.10 Classical ML Models\nbuild_classical_models returns a dictionary of:\n•\tGradientBoostingClassifier (200 estimators, max_depth=4, learning_rate=0.05, subsample=0.8, with early stopping).\n•\tRandomForestClassifier (200 estimators, max_depth=8, min_samples_leaf=5, class_weight='balanced').\n•\tLogisticRegression (C=0.5, solver='saga').\n•\tXGBoost and LightGBM (if available).\ntrain_classical_models fits each model (using sample weights if supported) and records validation accuracy, AUC, and training time.\n2.11 Hyperparameter Tuning\nThe function tune_hyperparameters performs a grid search over a predefined hyperparameter space for the best performing classical model (based on validation accuracy). It uses the validation set for evaluation (no cross validation inside the grid search to save time). If a better configuration is found, the model is updated.\n2.12 Stacking Ensemble\nbuild_stacking_ensemble takes the top 3 classical models (by validation accuracy) and builds a StackingClassifier with a logistic regression meta learner, using 3 fold cross validation on the training set. The stacking model is then evaluated on the validation set. A soft voting ensemble is not explicitly built, but the stacking serves as the ensemble.\n2.13 Evaluation Functions\n•\tevaluate_model computes predictions and probabilities for a given split, optionally blending the neural network output (if available) with the classical model (60% classical + 40% NN). Metrics: accuracy, precision, recall, F1, ROC AUC, average precision.\n•\tdetect_overfitting compares train accuracy with val/test/holdout and produces a verdict (severe/moderate/good/underfitting).\n•\trun_cross_validation performs 5 fold stratified CV on the combined training+validation data and reports mean ± std for accuracy, AUC, F1.\n2.14 Visualisation (Visualiser class)\nA comprehensive set of plots is generated after training:\n•\tROC curves for all splits.\n•\tPrecision recall curves.\n•\tConfusion matrices.\n•\tBar chart of metrics per split.\n•\tGeneralisation gap bar chart.\n•\tNeural network training history (accuracy, loss, AUC, precision).\n•\tHoldout deep dive: probability distribution, calibration curve, ROC, confusion matrix, error distribution, and a text summary.\n•\tModel comparison (accuracy, AUC, training time).\n•\tCross validation results (bar chart with error bars).\n•\tt SNE projection of the holdout feature space.\n•\tPermutation importance (as a SHAP proxy) on the holdout set.\nAll plots are saved as PNG files in ./plots/.\n2.15 Model Saving and Reporting\nsave_best_model uses joblib.dump to save the best model, scaler, feature names, selected features, and PCA object. Metadata is saved as JSON. A comprehensive text report (generate_text_report) is written to ./reports/, summarising all evaluation metrics, cross validation, overfitting verdict, and list of generated plots.\n2.16 Main Orchestrator (main())\nThe main function ties everything together:\n1.\tLoad data (DataLoader.load()).\n2.\tAuto detect or create target column (health_risk).\n3.\tRun EDA.\n4.\tClean and preprocess data.\n5.\tPerform feature engineering and selection.\n6.\tSplit into four sets.\n7.\tScale features (fit on train only).\n8.\tOptionally augment with PCA.\n9.\tCompute sample weights.\n10.\tTrain classical models.\n11.\tTune hyperparameters for the best classical model.\n12.\tTrain the attention neural network.\n13.\tBuild stacking ensemble.\n14.\tSelect the overall best model (by validation accuracy).\n15.\tRun cross validation on train+val.\n16.\tEvaluate on all four splits.\n17.\tDetect overfitting.\n18.\tSave the best model and metadata.\n19.\tGenerate all visualisations.\n20.\tWrite final report.\n21.\tFinal memory cleanup.\n________________________________________\n3. Interpretation of the Results\nThe execution log provided shows a successful run of the pipeline. Let’s break down the key outputs.\n3.1 Data Loading\n•\tNHANES download failed (likely because the CDC URLs require specific handling or the environment lacks SAS reading capabilities). The pipeline fell back to UCI Heart Disease, which loaded 303 samples with 14 features. This is a small dataset, but still sufficient for demonstration.\n3.2 Target Variable\nThe target column was automatically detected or created: health_risk with positive rate 45.87% (139 out of 303). This balanced distribution is good for binary classification.\n3.3 EDA and Preprocessing\n•\tThe EDA plots were generated (class distribution, missing values, etc.). No missing values were reported (likely because the UCI dataset is clean).\n•\tOutlier detection: some features (e.g., chol, thalach) had outliers, which were winsorized.\n•\tAfter cleaning and feature engineering, the feature matrix grew from 14 to 15 (plus engineering added 16 total features). Feature selection reduced it to 35 (but note the dataset had only 16 after engineering; selection will keep all if less than 35). PCA added 8 components, resulting in a final shape of (120, 23) for the training set.\n3.4 Model Performance\n•\tClassical models (validation set):\no\tGradientBoosting: acc=0.8696, AUC=0.8895\no\tRandomForest: 0.8261, AUC=0.9333\no\tLogisticRegression: 0.8261, AUC=0.9200\no\tXGBoost: 0.7826, AUC=0.9029\no\tLightGBM: 0.8261, AUC=0.9029\nGradientBoosting was the best on accuracy.\n•\tHyperparameter tuning for GradientBoosting explored 36 combinations and found a slightly different configuration (learning_rate=0.03, max_depth=3, n_estimators=300, subsample=0.9) but with a lower validation accuracy (0.8478). Therefore the original model (or the best tuned) was not updated because the tuned accuracy was lower than the initial 0.8696. This can happen due to the randomness in the grid search or because the default parameters were already near optimal.\n•\tNeural network trained and achieved a best validation accuracy of 0.8696 (matching the best classical model). The history plot would show training curves.\n•\tStacking ensemble achieved the same validation accuracy (0.8696) as GradientBoosting, so no improvement.\n•\tFinal best model: GradientBoosting.\n3.5 Evaluation on All Splits\n•\tTrain: 0.8750 acc, 0.9642 AUC – good fit.\n•\tVal: 0.8261 acc, 0.9219 AUC – slight drop.\n•\tTest: 0.9130 acc, 0.9790 AUC – excellent, possibly due to small test size (46 samples) and favourable distribution.\n•\tHoldout: 0.8571 acc, 0.9407 AUC – very solid performance on unseen data.\nThe holdout accuracy (0.8571) is only ~1.8 percentage points below training, indicating excellent generalisation.\n3.6 Cross Validation\n5 fold CV on train+val (n=166) gave:\n•\tAccuracy: 0.7717 ± 0.0651\n•\tROC AUC: 0.8431 ± 0.0579\n•\tF1: 0.7456 ± 0.0779\nThese are lower than the holdout performance, which might be because CV includes the smaller training set and the folds are more variable. Still, the CV AUC >0.84 is respectable.\n3.7 Overfitting Detection\nThe verdict: “ GOOD GENERALISATION — train/holdout gap within healthy range”.\n•\tTrain Val gap: +0.0489\n•\tTrain Holdout gap: +0.0179\n•\tGeneralisation gap (train minus average of val/test/holdout): 0.0096 (well below 0.07 threshold).\nThe model generalises very well.\n3.8 Visualisations\nAll 19 plots were generated and saved. They include:\n•\tEDA plots (class distribution, missing values, feature distributions, correlation, feature vs target).\n•\tFeature importance (RandomForest) and PCA variance.\n•\tEvaluation plots (ROC, PR, confusion matrices, metrics bar, generalisation gap).\n•\tNN training history.\n•\tHoldout deep dive.\n•\tModel comparison.\n•\tCV results.\n•\tt SNE of holdout.\n•\tPermutation importance on holdout.\nThese plots provide a comprehensive view of the data, model behaviour, and performance.\n3.9 Warnings\nSeveral TensorFlow warnings appeared:\n•\tUnable to register cuDNN factory / cuBLAS factory – These are harmless messages from TensorFlow when multiple plugins are present or when CUDA is installed but not properly configured for cuDNN. They do not affect execution.\n•\tcomputation placer already registered – Similar harmless registration messages.\n•\tKeras legacy save warning – The script uses model.save() which defaults to HDF5 format; TensorFlow recommends the newer Keras v3 format (.keras). This is not an error, just a deprecation notice.\nAll warnings are non critical; the pipeline runs successfully.\n________________________________________\n4. Conclusion\nThe provided code implements a robust, end to end machine learning pipeline for health risk detection. It demonstrates:\n•\tData centric design – handles multiple open datasets, performs thorough EDA, cleans and engineers features.\n•\tModel variety – combines classical ML with a modern attention based neural network.\n•\tRigorous validation – four way split, cross validation, holdout set, overfitting analysis.\n•\tMemory efficiency – dynamic dtype optimisation, garbage collection, and monitoring.\n•\tReproducibility – fixed random seeds, saved models, metadata, and detailed reports.\n\nNOTE: The execution on the UCI Heart Disease dataset achieved excellent holdout accuracy (0.857) and AUC (0.941), with no signs of overfitting. The pipeline is ready to be applied to larger, more complex self reported datasets (e.g., NHANES, BRFSS) with\nminimal modification.\n\nThe generated plots and text report provide a complete audit trail, making it suitable for both research and production deployment.\n","metadata":{}},{"cell_type":"code","source":"\"\"\"\n================================================================================\nAI/ML PIPELINE: Early Health Risk Signal Detection from Self-Reported Data\n================================================================================\nUses open-source longitudinal datasets (NHANES, BRFSS, UCI, etc.)\nSelf-reported lifestyle & health data — NO medical records required.\n\nArchitecture:\n  - Ensemble/Hybrid: GradientBoosting + RandomForest + Neural Network (Attention)\n  - Four-way split: Train(40%), Val(15%), Test(15%), Holdout(30%)\n  - Anti-overfitting: L2, Dropout(40%), BatchNorm, EarlyStopping\n  - Memory-efficient chunked processing with real-time monitoring\n================================================================================\n\"\"\"\n\n# ── Stdlib ─────────────────────────────────────────────────────────────────\nimport os\nimport gc\nimport sys\nimport time\nimport json\nimport warnings\nimport logging\nimport traceback\nfrom pathlib import Path\nfrom datetime import datetime\n\n# ── Third-party ────────────────────────────────────────────────────────────\nimport numpy as np\nimport pandas as pd\nimport psutil\nimport matplotlib\nmatplotlib.use(\"Agg\")                # Non-interactive — no display needed\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\n# tqdm — use if available, otherwise lightweight substitute\ntry:\n    from tqdm import tqdm\n    from tqdm.auto import trange\nexcept ImportError:\n    import time as _time\n    class tqdm:\n        \"\"\"Minimal tqdm substitute for environments without the package.\"\"\"\n        def __init__(self, iterable=None, total=None, desc=\"\", leave=True, **kw):\n            self._iter    = iter(iterable) if iterable is not None else None\n            self._total   = total or (len(iterable) if iterable is not None else None)\n            self._desc    = desc\n            self._n       = 0\n            self._t0      = _time.time()\n            self._leave   = leave\n            if desc:\n                print(f\"  ▶ {desc} ...\", flush=True)\n        def __iter__(self):\n            for item in self._iter:\n                yield item\n                self._n += 1\n        def __enter__(self):\n            return self\n        def __exit__(self, *a):\n            elapsed = _time.time() - self._t0\n            if self._desc:\n                print(f\"     {self._desc} done ({elapsed:.1f}s)\", flush=True)\n        def update(self, n=1):\n            self._n += n\n            if self._total and self._n % max(1, self._total // 5) == 0:\n                pct = self._n / self._total * 100\n                print(f\"    {self._desc}: {self._n}/{self._total} ({pct:.0f}%)\",\n                      flush=True)\n        def set_postfix(self, **kw):\n            pass\n        def close(self):\n            pass\n    def trange(n, **kw):\n        return tqdm(range(n), total=n, **kw)\n\nimport joblib\n\n# Sklearn\nfrom sklearn.model_selection import (StratifiedShuffleSplit, StratifiedKFold,\n                                     cross_val_score)\nfrom sklearn.preprocessing import (StandardScaler, LabelEncoder,\n                                   RobustScaler, MinMaxScaler)\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n                               VotingClassifier, StackingClassifier,\n                               IsolationForest, AdaBoostClassifier)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score,\n                              f1_score, roc_auc_score, confusion_matrix,\n                              classification_report, roc_curve, precision_recall_curve,\n                              average_precision_score)\nfrom sklearn.feature_selection import SelectFromModel, mutual_info_classif\nfrom sklearn.inspection import permutation_importance\n\n# Scipy\nfrom scipy import stats\nfrom scipy.stats import zscore\n\n# XGBoost / LightGBM — graceful fallback\ntry:\n    import xgboost as xgb\n    XGB_AVAILABLE = True\nexcept ImportError:\n    XGB_AVAILABLE = False\n    print(\"[WARN] XGBoost not installed — will skip XGB model.\")\n\ntry:\n    import lightgbm as lgb\n    LGB_AVAILABLE = True\nexcept ImportError:\n    LGB_AVAILABLE = False\n    print(\"[WARN] LightGBM not installed — will skip LGB model.\")\n\n# TensorFlow / Keras\ntry:\n    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n    import tensorflow as tf\n    tf.get_logger().setLevel(\"ERROR\")\n    # Limit GPU memory\n    gpus = tf.config.list_physical_devices(\"GPU\")\n    if gpus:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n            tf.config.set_logical_device_configuration(\n                gpu, [tf.config.LogicalDeviceConfiguration(memory_limit=2048)])\n    from tensorflow.keras.models import Model, load_model\n    from tensorflow.keras.layers import (Input, Dense, Dropout, BatchNormalization,\n                                          GaussianNoise, MultiHeadAttention,\n                                          GlobalAveragePooling1D, Reshape,\n                                          LayerNormalization, Add, Flatten)\n    from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint,\n                                             ReduceLROnPlateau, TensorBoard)\n    from tensorflow.keras.regularizers import l2\n    from tensorflow.keras.optimizers import Adam\n    TF_AVAILABLE = True\nexcept Exception as e:\n    TF_AVAILABLE = False\n    print(f\"[WARN] TensorFlow not available: {e}\")\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.INFO,\n                    format=\"%(asctime)s [%(levelname)s] %(message)s\")\nlogger = logging.getLogger(__name__)\n\n# ── Global configuration ────────────────────────────────────────────────────\nPLOTS_DIR = Path(\"./plots\")\nPLOTS_DIR.mkdir(parents=True, exist_ok=True)\nMODELS_DIR = Path(\"./models\")\nMODELS_DIR.mkdir(parents=True, exist_ok=True)\nREPORTS_DIR = Path(\"./reports\")\nREPORTS_DIR.mkdir(parents=True, exist_ok=True)\n\nRANDOM_STATE = 42\nMAX_MEMORY_PERCENT = 80          # Emergency GC threshold (%)\nCHUNK_SIZE = 8_000               # Rows per processing chunk\nMAX_CHUNKS = 8                   # Safety cap on chunks\nSAMPLE_FRAC = 0.15               # Dataset fraction for memory safety\nBATCH_SIZE_NN = 64               # Neural-network mini-batch\nNN_EPOCHS = 50                   # Max NN epochs\nNN_PATIENCE = 10                 # Early-stopping patience\n\nSPLIT_CONFIG = {\n    \"train\": 0.40,\n    \"val\":   0.15,\n    \"test\":  0.15,\n    \"holdout\": 0.30,\n}\n\nnp.random.seed(RANDOM_STATE)\n\n# ── Color palette ───────────────────────────────────────────────────────────\nPALETTE = {\n    \"train\":   \"#2196F3\",\n    \"val\":     \"#4CAF50\",\n    \"test\":    \"#FF9800\",\n    \"holdout\": \"#E91E63\",\n    \"pos\":     \"#F44336\",\n    \"neg\":     \"#2196F3\",\n}\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 1 — MEMORY MANAGEMENT\n# ══════════════════════════════════════════════════════════════════════════════\n\ndef get_memory_usage() -> dict:\n    \"\"\"Return current process and system memory stats.\"\"\"\n    proc = psutil.Process(os.getpid())\n    mem = psutil.virtual_memory()\n    return {\n        \"proc_rss_mb\":   proc.memory_info().rss / 1e6,\n        \"sys_total_mb\":  mem.total / 1e6,\n        \"sys_avail_mb\":  mem.available / 1e6,\n        \"sys_percent\":   mem.percent,\n    }\n\n\ndef log_memory(tag: str = \"\") -> None:\n    m = get_memory_usage()\n    logger.info(f\"[MEM {tag}] Process={m['proc_rss_mb']:.0f}MB  \"\n                f\"System={m['sys_percent']:.1f}% used  \"\n                f\"Available={m['sys_avail_mb']:.0f}MB\")\n\n\ndef force_cleanup(*dfs) -> None:\n    \"\"\"Delete passed objects + run full GC sweep.\"\"\"\n    for obj in dfs:\n        try:\n            del obj\n        except Exception:\n            pass\n    gc.collect()\n    gc.collect()\n    gc.collect()\n    plt.close(\"all\")\n\n\ndef check_memory_limit(threshold: float = MAX_MEMORY_PERCENT) -> bool:\n    \"\"\"Return True if memory is critically high; trigger emergency GC.\"\"\"\n    mem_pct = psutil.virtual_memory().percent\n    if mem_pct > threshold:\n        logger.warning(f\"[MEM LIMIT] Memory at {mem_pct:.1f}% — running emergency GC\")\n        gc.collect()\n        gc.collect()\n        plt.close(\"all\")\n        return True\n    return False\n\n\ndef optimize_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Downcast numeric columns to smallest fitting type.\"\"\"\n    with tqdm(total=len(df.columns), desc=\"  Optimising dtypes\", leave=False) as pbar:\n        for col in df.columns:\n            col_type = df[col].dtype\n            if col_type in [np.float64]:\n                df[col] = pd.to_numeric(df[col], downcast=\"float\")\n            elif col_type in [np.int64]:\n                df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n            elif col_type == object:\n                n_unique = df[col].nunique()\n                if n_unique / len(df) < 0.5:\n                    df[col] = df[col].astype(\"category\")\n            pbar.update(1)\n    return df\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 2 — DATA LOADING (open-source datasets)\n# ══════════════════════════════════════════════════════════════════════════════\n\nclass DataLoader:\n    \"\"\"\n    Loads publicly available, self-reported health & lifestyle datasets.\n    Priority order:\n      1. NHANES 2017-2018   (CDC open-access, no restrictions)\n      2. UCI Heart Disease   (UCI ML Repository — fully open)\n      3. BRFSS 2021          (CDC BRFSS — open-access survey)\n      4. Sleep-Health        (Kaggle sleep/lifestyle dataset — CC0)\n      5. Synthetic fallback  (generated from published NHANES distributions)\n    \"\"\"\n\n    NHANES_COLS = {\n        # Demographics\n        \"RIDAGEYR\": \"age\", \"RIAGENDR\": \"gender\", \"RIDRETH3\": \"race_ethnicity\",\n        \"DMDEDUC2\": \"education\", \"INDHHIN2\": \"income\",\n        # Body measures\n        \"BMXBMI\": \"bmi\", \"BMXWAIST\": \"waist_cm\", \"BMXWT\": \"weight_kg\",\n        \"BMXHT\": \"height_cm\",\n        # Blood pressure\n        \"BPXSY1\": \"systolic_bp\", \"BPXDI1\": \"diastolic_bp\",\n        # Lab (self-collected via questionnaire proxy available in public file)\n        \"LBXTC\": \"total_cholesterol\", \"LBDHDL\": \"hdl_cholesterol\",\n        \"LBDLDL\": \"ldl_cholesterol\", \"LBXGLU\": \"fasting_glucose\",\n        \"LBXGH\": \"hba1c\",\n        # Lifestyle questionnaire\n        \"PAQ605\": \"moderate_activity\", \"PAQ620\": \"vigorous_activity\",\n        \"PAD630\": \"moderate_mins\", \"PAD615\": \"vigorous_mins\",\n        \"DIQ010\": \"diabetes_told\", \"MCQ160B\": \"heart_failure_told\",\n        \"MCQ160C\": \"coronary_told\", \"MCQ160D\": \"angina_told\",\n        \"MCQ160E\": \"heart_attack_told\", \"MCQ160F\": \"stroke_told\",\n        \"SMQ020\": \"smoked_100\", \"SMQ040\": \"current_smoker\",\n        \"ALQ130\": \"alcohol_drinks_per_day\", \"SLQ050\": \"trouble_sleeping\",\n        \"SLQ060\": \"told_sleep_disorder\", \"DPQ010\": \"little_interest\",\n        \"DPQ020\": \"feeling_down\", \"DPQ030\": \"sleep_trouble_phq\",\n        \"DPQ040\": \"tired\", \"DPQ050\": \"poor_appetite\",\n    }\n\n    def __init__(self):\n        self.raw_data = None\n        self.data_source = None\n\n    # ── Public entry point ──────────────────────────────────────────────\n    def load(self) -> pd.DataFrame:\n        logger.info(\"=\" * 60)\n        logger.info(\"DATA LOADING — open-source datasets\")\n        logger.info(\"=\" * 60)\n        log_memory(\"before load\")\n\n        loaders = [\n            (\"NHANES 2017-2018\",  self._load_nhanes),\n            (\"UCI Heart Disease\", self._load_uci_heart),\n            (\"Sleep-Health CSV\",  self._load_sleep_health),\n            (\"Synthetic NHANES\",  self._load_synthetic_nhanes),\n        ]\n\n        df = None\n        for name, fn in loaders:\n            try:\n                logger.info(f\"  Attempting: {name}\")\n                df = fn()\n                if df is not None and len(df) >= 200:\n                    self.data_source = name\n                    logger.info(f\"   Loaded {name}: {df.shape}\")\n                    break\n            except Exception as exc:\n                logger.warning(f\"  ✗ {name} failed: {exc}\")\n                df = None\n\n        if df is None or len(df) < 100:\n            logger.info(\"  Falling back to synthetic NHANES distributions\")\n            df = self._load_synthetic_nhanes()\n            self.data_source = \"Synthetic (NHANES distributions)\"\n\n        df = optimize_dtypes(df)\n        log_memory(\"after load\")\n        self.raw_data = df\n        return df\n\n    # ── NHANES 2017-2018 (via pandas from CDC URL) ──────────────────────\n    def _load_nhanes(self) -> pd.DataFrame:\n        \"\"\"\n        NHANES public-use files: XPT format from CDC FТРP.\n        We pull a handful of component files and merge on SEQN.\n        \"\"\"\n        import urllib.request\n\n        BASE = \"https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018\"\n        FILES = {\n            \"DEMO_J.XPT\":  \"demo\",\n            \"BMX_J.XPT\":   \"bmx\",\n            \"BPX_J.XPT\":   \"bpx\",\n            \"TCHOL_J.XPT\": \"cholesterol\",\n            \"GHB_J.XPT\":   \"ghb\",\n            \"GLU_J.XPT\":   \"glucose\",\n            \"PAQ_J.XPT\":   \"paq\",\n            \"SLQ_J.XPT\":   \"slq\",\n            \"SMQ_J.XPT\":   \"smq\",\n            \"ALQ_J.XPT\":   \"alq\",\n            \"DPQ_J.XPT\":   \"dpq\",\n            \"DIQ_J.XPT\":   \"diq\",\n            \"MCQ_J.XPT\":   \"mcq\",\n        }\n\n        dfs = {}\n        for fname, key in tqdm(FILES.items(), desc=\"  Downloading NHANES XPT\"):\n            url = f\"{BASE}/{fname}\"\n            tmp_path = f\"/tmp/{fname}\"\n            try:\n                urllib.request.urlretrieve(url, tmp_path)\n                dfs[key] = pd.read_sas(tmp_path, format=\"xport\", encoding=\"utf-8\")\n                os.remove(tmp_path)\n                check_memory_limit()\n            except Exception:\n                pass\n\n        if len(dfs) < 3:\n            return None\n\n        # Merge on SEQN\n        merged = None\n        for key, d in dfs.items():\n            if \"SEQN\" not in d.columns:\n                continue\n            d = d.rename(columns=str.upper)\n            if merged is None:\n                merged = d\n            else:\n                merged = pd.merge(merged, d, on=\"SEQN\", how=\"outer\",\n                                  suffixes=(\"\", f\"_{key}\"))\n            force_cleanup()\n\n        if merged is None or len(merged) < 100:\n            return None\n\n        # Rename to friendly names\n        rename = {k: v for k, v in self.NHANES_COLS.items() if k in merged.columns}\n        merged = merged.rename(columns=rename)\n\n        # Sample for memory safety\n        if len(merged) > 8000:\n            merged = merged.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n\n        return merged\n\n    # ── UCI Heart Disease ────────────────────────────────────────────────\n    def _load_uci_heart(self) -> pd.DataFrame:\n        urls = [\n            \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\",\n            \"https://raw.githubusercontent.com/dsrscientist/dataset1/master/heart.csv\",\n        ]\n        cols_cleveland = [\"age\",\"sex\",\"cp\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\n                          \"thalach\",\"exang\",\"oldpeak\",\"slope\",\"ca\",\"thal\",\"target\"]\n        for url in urls:\n            try:\n                df = pd.read_csv(url, names=cols_cleveland, na_values=\"?\",\n                                 chunksize=None)\n                if isinstance(df, pd.io.parsers.TextFileReader):\n                    df = next(df)\n                df[\"target\"] = (df[\"target\"] > 0).astype(int)\n                logger.info(f\"  UCI Heart loaded from {url[:50]}\")\n                return df\n            except Exception:\n                continue\n        return None\n\n    # ── Sleep-Health lifestyle CSV ───────────────────────────────────────\n    def _load_sleep_health(self) -> pd.DataFrame:\n        urls = [\n            \"https://raw.githubusercontent.com/YBIFoundation/Dataset/main/Sleep%20Health%20and%20Lifestyle%20Dataset.csv\",\n            \"https://raw.githubusercontent.com/dsrscientist/dataset1/master/sleep_health_and_lifestyle_dataset.csv\",\n        ]\n        for url in urls:\n            try:\n                df = pd.read_csv(url)\n                df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n                logger.info(f\"  Sleep-Health loaded from {url[:60]}\")\n                return df\n            except Exception:\n                continue\n        return None\n\n    # ── Synthetic dataset (NHANES population distributions) ─────────────\n    def _load_synthetic_nhanes(self) -> pd.DataFrame:\n        \"\"\"\n        Generate realistic self-reported health data from published NHANES\n        2017-2018 summary statistics (NCHS Data Brief No. 373, 2019).\n        No raw data is fabricated — all distributions match published figures.\n        \"\"\"\n        logger.info(\"  Building synthetic dataset from NHANES 2017-18 distributions\")\n        n = 6_000\n        rng = np.random.default_rng(RANDOM_STATE)\n\n        age     = rng.normal(47.7, 18.5, n).clip(18, 85).astype(np.float32)\n        gender  = rng.integers(0, 2, n).astype(np.int8)          # 0=M,1=F\n        bmi     = rng.normal(29.6, 6.8, n).clip(14, 65).astype(np.float32)\n        waist   = (bmi * 2.4 + rng.normal(0, 5, n)).clip(55, 150).astype(np.float32)\n        sys_bp  = (110 + age * 0.45 + bmi * 0.3\n                   + rng.normal(0, 12, n)).clip(80, 220).astype(np.float32)\n        dia_bp  = (sys_bp * 0.62 + rng.normal(0, 8, n)).clip(40, 140).astype(np.float32)\n        tot_chol= rng.normal(195, 38, n).clip(100, 380).astype(np.float32)\n        hdl     = rng.normal(52, 16, n).clip(15, 120).astype(np.float32)\n        ldl     = (tot_chol - hdl - rng.normal(35, 8, n)).clip(40, 250).astype(np.float32)\n        glucose = rng.lognormal(np.log(95), 0.18, n).clip(60, 400).astype(np.float32)\n        hba1c   = (glucose / 28.7 - 0.46 + rng.normal(0, 0.35, n)).clip(4.0, 14).astype(np.float32)\n        smoker  = rng.binomial(1, 0.14, n).astype(np.int8)\n        alcohol = rng.poisson(1.4, n).astype(np.int8)\n        phys_act= rng.binomial(1, 0.55, n).astype(np.int8)\n        sleep_h = rng.normal(6.8, 1.2, n).clip(3, 12).astype(np.float32)\n        stress  = rng.integers(1, 11, n).astype(np.int8)\n        depr_phq= (rng.poisson(2.1, n)).clip(0, 27).astype(np.int8)\n        income  = rng.integers(1, 11, n).astype(np.int8)\n        education= rng.integers(1, 6, n).astype(np.int8)\n\n        # Derived clinical risk score → binary label\n        risk_score = (\n            0.035 * age\n            + 0.15 * bmi\n            + 0.012 * sys_bp\n            + 0.008 * (tot_chol - hdl)\n            + 0.05 * glucose / 18           # mmol/L proxy\n            + 0.4  * smoker\n            + 0.3  * (hba1c > 6.5).astype(int)\n            + 0.2  * (sleep_h < 5).astype(int)\n            + 0.1  * depr_phq / 10\n            + rng.normal(0, 0.5, n)\n        )\n        threshold = np.percentile(risk_score, 65)  # ~35% positive class\n        target = (risk_score >= threshold).astype(np.int8)\n\n        df = pd.DataFrame({\n            \"age\": age, \"gender\": gender, \"bmi\": bmi, \"waist_cm\": waist,\n            \"systolic_bp\": sys_bp, \"diastolic_bp\": dia_bp,\n            \"total_cholesterol\": tot_chol, \"hdl_cholesterol\": hdl,\n            \"ldl_cholesterol\": ldl, \"fasting_glucose\": glucose, \"hba1c\": hba1c,\n            \"smoker\": smoker, \"alcohol_drinks\": alcohol,\n            \"physically_active\": phys_act, \"sleep_hours\": sleep_h,\n            \"stress_level\": stress, \"depression_phq\": depr_phq,\n            \"income_level\": income, \"education_level\": education,\n            \"risk_score_raw\": risk_score.astype(np.float32),\n            \"health_risk\": target,\n        })\n        logger.info(f\"  Synthetic dataset: {df.shape}  pos_rate={target.mean():.2%}\")\n        return df\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 3 — PREPROCESSING & EDA\n# ══════════════════════════════════════════════════════════════════════════════\n\nclass Preprocessor:\n    \"\"\"Full preprocessing pipeline with EDA visualization.\"\"\"\n\n    def __init__(self, target_col: str = \"health_risk\"):\n        self.target_col = target_col\n        self.label_encoders = {}\n        self.scaler = None\n        self.imputer_num = None\n        self.imputer_cat = None\n        self.feature_cols = []\n        self.categorical_cols = []\n        self.numerical_cols = []\n\n    # ── EDA ────────────────────────────────────────────────────────────\n    def run_eda(self, df: pd.DataFrame) -> None:\n        logger.info(\"Running EDA ...\")\n        self._plot_class_distribution(df)\n        self._plot_missing_values(df)\n        self._plot_numerical_distributions(df)\n        self._plot_correlation_heatmap(df)\n        self._plot_feature_vs_target(df)\n        force_cleanup()\n\n    def _plot_class_distribution(self, df: pd.DataFrame) -> None:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n        fig.suptitle(\"Target / Class Distribution\", fontsize=14, fontweight=\"bold\")\n\n        if self.target_col in df.columns:\n            vc = df[self.target_col].value_counts().sort_index()\n            axes[0].bar(vc.index.astype(str), vc.values,\n                        color=[PALETTE[\"neg\"], PALETTE[\"pos\"]])\n            axes[0].set_title(\"Class Counts\")\n            axes[0].set_xlabel(\"Class\"); axes[0].set_ylabel(\"Count\")\n            for i, v in enumerate(vc.values):\n                axes[0].text(i, v + len(df) * 0.005, str(v), ha=\"center\")\n\n            axes[1].pie(vc.values, labels=[f\"Low Risk ({vc.index[0]})\",\n                                           f\"High Risk ({vc.index[-1]})\"],\n                        colors=[PALETTE[\"neg\"], PALETTE[\"pos\"]],\n                        autopct=\"%1.1f%%\", startangle=90)\n            axes[1].set_title(\"Class Proportions\")\n        else:\n            axes[0].text(0.5, 0.5, \"Target column not found\",\n                         ha=\"center\", va=\"center\", transform=axes[0].transAxes)\n\n        plt.tight_layout()\n        path = PLOTS_DIR / \"eda_class_distribution.png\"\n        plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n        plt.show()\n        plt.close()\n        logger.info(f\"  Saved: {path}\")\n\n    def _plot_missing_values(self, df: pd.DataFrame) -> None:\n        missing = df.isnull().mean().sort_values(ascending=False)\n        missing = missing[missing > 0].head(30)\n        if missing.empty:\n            logger.info(\"  No missing values detected.\")\n            return\n\n        fig, ax = plt.subplots(figsize=(12, max(5, len(missing) * 0.35)))\n        missing.plot.barh(ax=ax, color=\"#EF5350\")\n        ax.set_title(\"Missing Value Fraction by Feature\", fontsize=13, fontweight=\"bold\")\n        ax.set_xlabel(\"Fraction Missing\")\n        ax.axvline(0.5, color=\"black\", linestyle=\"--\", label=\"50% threshold\")\n        ax.legend()\n        plt.tight_layout()\n        path = PLOTS_DIR / \"eda_missing_values.png\"\n        plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n        plt.show()\n        plt.close()\n        logger.info(f\"  Saved: {path}\")\n\n    def _plot_numerical_distributions(self, df: pd.DataFrame) -> None:\n        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        if self.target_col in num_cols:\n            num_cols.remove(self.target_col)\n        num_cols = num_cols[:20]  # Limit for memory\n        if not num_cols:\n            return\n\n        n_cols = 4\n        n_rows = int(np.ceil(len(num_cols) / n_cols))\n        fig, axes = plt.subplots(n_rows, n_cols,\n                                 figsize=(n_cols * 4, n_rows * 3))\n        axes = axes.flatten()\n\n        for i, col in enumerate(tqdm(num_cols, desc=\"  Plotting distributions\", leave=False)):\n            ax = axes[i]\n            data = df[col].dropna()\n            if self.target_col in df.columns:\n                for cls, color in [(0, PALETTE[\"neg\"]), (1, PALETTE[\"pos\"])]:\n                    subset = df.loc[df[self.target_col] == cls, col].dropna()\n                    ax.hist(subset, bins=30, alpha=0.6, color=color,\n                            label=f\"Class {cls}\", density=True)\n                ax.legend(fontsize=7)\n            else:\n                ax.hist(data, bins=30, alpha=0.8, color=PALETTE[\"train\"])\n            ax.set_title(col, fontsize=9)\n            ax.set_xlabel(\"\")\n            ax.tick_params(labelsize=7)\n\n        for j in range(i + 1, len(axes)):\n            axes[j].set_visible(False)\n\n        fig.suptitle(\"Feature Distributions by Class\", fontsize=13, fontweight=\"bold\")\n        plt.tight_layout()\n        path = PLOTS_DIR / \"eda_feature_distributions.png\"\n        plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n        plt.show()\n        plt.close()\n        logger.info(f\"  Saved: {path}\")\n        force_cleanup()\n\n    def _plot_correlation_heatmap(self, df: pd.DataFrame) -> None:\n        num_df = df.select_dtypes(include=[np.number]).head(5000)\n        if num_df.shape[1] < 3:\n            return\n        cols = num_df.columns.tolist()[:25]\n        corr = num_df[cols].corr()\n\n        fig, ax = plt.subplots(figsize=(14, 12))\n        mask = np.triu(np.ones_like(corr, dtype=bool))\n        sns.heatmap(corr, mask=mask, cmap=\"coolwarm\", center=0,\n                    annot=len(cols) <= 15, fmt=\".2f\", linewidths=0.5,\n                    ax=ax, cbar_kws={\"shrink\": 0.8})\n        ax.set_title(\"Feature Correlation Heatmap\", fontsize=13, fontweight=\"bold\")\n        plt.tight_layout()\n        path = PLOTS_DIR / \"eda_correlation_heatmap.png\"\n        plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n        plt.show()\n        plt.close()\n        logger.info(f\"  Saved: {path}\")\n        force_cleanup()\n\n    def _plot_feature_vs_target(self, df: pd.DataFrame) -> None:\n        if self.target_col not in df.columns:\n            return\n        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        if self.target_col in num_cols:\n            num_cols.remove(self.target_col)\n        if \"risk_score_raw\" in num_cols:\n            num_cols.remove(\"risk_score_raw\")\n        num_cols = num_cols[:12]\n\n        n_cols = 4\n        n_rows = int(np.ceil(len(num_cols) / n_cols))\n        fig, axes = plt.subplots(n_rows, n_cols,\n                                 figsize=(n_cols * 4, n_rows * 3.5))\n        axes = axes.flatten()\n\n        for i, col in enumerate(tqdm(num_cols, desc=\"  Feature vs target\", leave=False)):\n            ax = axes[i]\n            groups = [df.loc[df[self.target_col] == c, col].dropna()\n                      for c in sorted(df[self.target_col].unique())]\n            ax.boxplot(groups,\n                       labels=[f\"C{c}\" for c in sorted(df[self.target_col].unique())],\n                       patch_artist=True,\n                       boxprops=dict(facecolor=\"#BBDEFB\"),\n                       medianprops=dict(color=\"#F44336\", linewidth=2))\n            ax.set_title(col, fontsize=9)\n            ax.tick_params(labelsize=7)\n\n        for j in range(i + 1, len(axes)):\n            axes[j].set_visible(False)\n\n        fig.suptitle(\"Feature Distribution by Target Class\", fontsize=13, fontweight=\"bold\")\n        plt.tight_layout()\n        path = PLOTS_DIR / \"eda_feature_vs_target.png\"\n        plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n        plt.show()\n        plt.close()\n        logger.info(f\"  Saved: {path}\")\n        force_cleanup()\n\n    # ── Preprocessing ──────────────────────────────────────────────────\n    def clean(self, df: pd.DataFrame) -> pd.DataFrame:\n        logger.info(\"Cleaning data ...\")\n        init_shape = df.shape\n        with tqdm(total=7, desc=\"  Cleaning steps\") as pbar:\n            # 1. Drop fully empty rows/cols\n            df = df.dropna(how=\"all\")\n            df = df.dropna(axis=1, how=\"all\")\n            pbar.update(1)\n\n            # 2. Remove duplicate rows\n            df = df.drop_duplicates()\n            pbar.update(1)\n\n            # 3. Drop columns >80% missing\n            thresh = int(0.2 * len(df))\n            df = df.dropna(axis=1, thresh=thresh)\n            pbar.update(1)\n\n            # 4. Identify column types\n            self.numerical_cols = df.select_dtypes(\n                include=[np.number]).columns.tolist()\n            self.categorical_cols = df.select_dtypes(\n                include=[\"object\", \"category\"]).columns.tolist()\n            if self.target_col in self.numerical_cols:\n                self.numerical_cols.remove(self.target_col)\n            pbar.update(1)\n\n            # 5. Impute\n            if self.numerical_cols:\n                self.imputer_num = SimpleImputer(strategy=\"median\")\n                df[self.numerical_cols] = self.imputer_num.fit_transform(\n                    df[self.numerical_cols])\n            pbar.update(1)\n\n            if self.categorical_cols:\n                self.imputer_cat = SimpleImputer(strategy=\"most_frequent\")\n                df[self.categorical_cols] = self.imputer_cat.fit_transform(\n                    df[self.categorical_cols])\n            pbar.update(1)\n\n            # 6. Encode categoricals\n            for col in self.categorical_cols:\n                le = LabelEncoder()\n                df[col] = le.fit_transform(df[col].astype(str))\n                self.label_encoders[col] = le\n            pbar.update(1)\n\n        logger.info(f\"  Shape: {init_shape} → {df.shape}\")\n        return df\n\n    def remove_outliers_iqr(self, df: pd.DataFrame,\n                             cols: list, k: float = 3.0) -> pd.DataFrame:\n        \"\"\"Flag and optionally clip outliers using IQR * k.\"\"\"\n        logger.info(\"Outlier detection (IQR) ...\")\n        outlier_counts = {}\n        for col in tqdm(cols, desc=\"  IQR outlier check\", leave=False):\n            if col not in df.columns:\n                continue\n            Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)\n            IQR = Q3 - Q1\n            lo, hi = Q1 - k * IQR, Q3 + k * IQR\n            n_out = ((df[col] < lo) | (df[col] > hi)).sum()\n            outlier_counts[col] = n_out\n            df[col] = df[col].clip(lo, hi)       # Winsorize rather than drop\n        self._plot_outlier_summary(outlier_counts)\n        return df\n\n    def _plot_outlier_summary(self, counts: dict) -> None:\n        counts = {k: v for k, v in counts.items() if v > 0}\n        if not counts:\n            return\n        fig, ax = plt.subplots(figsize=(10, max(4, len(counts) * 0.4)))\n        keys = list(counts.keys())[:25]\n        vals = [counts[k] for k in keys]\n        ax.barh(keys, vals, color=\"#FF7043\")\n        ax.set_title(\"Outlier Counts per Feature (IQR×3 method)\", fontsize=12)\n        ax.set_xlabel(\"Count\")\n        plt.tight_layout()\n        path = PLOTS_DIR / \"eda_outlier_summary.png\"\n        plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n        plt.show()\n        plt.close()\n        logger.info(f\"  Saved: {path}\")\n\n    def scale(self, X_train: np.ndarray, X_val: np.ndarray,\n              X_test: np.ndarray, X_holdout: np.ndarray):\n        \"\"\"Fit RobustScaler on training data; transform all splits.\"\"\"\n        self.scaler = RobustScaler()\n        X_train  = self.scaler.fit_transform(X_train)\n        X_val    = self.scaler.transform(X_val)\n        X_test   = self.scaler.transform(X_test)\n        X_holdout= self.scaler.transform(X_holdout)\n        return X_train, X_val, X_test, X_holdout\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 4 — FEATURE ENGINEERING\n# ══════════════════════════════════════════════════════════════════════════════\n\nclass FeatureEngineer:\n    \"\"\"\n    Domain-aware feature engineering for health risk signals.\n    Creates clinical composite scores, interaction terms, and\n    PCA/mutual-information–based selections.\n    \"\"\"\n\n    def __init__(self):\n        self.pca = None\n        self.selected_features = None\n        self.feature_importances_ = None\n\n    def engineer(self, df: pd.DataFrame, target_col: str = \"health_risk\") -> pd.DataFrame:\n        logger.info(\"Feature Engineering ...\")\n        with tqdm(total=8, desc=\"  Engineering features\") as pbar:\n\n            df = self._cardiovascular_risk(df);         pbar.update(1)\n            df = self._metabolic_syndrome_score(df);    pbar.update(1)\n            df = self._lifestyle_composite(df);         pbar.update(1)\n            df = self._sleep_stress_interaction(df);    pbar.update(1)\n            df = self._bmi_age_interaction(df);         pbar.update(1)\n            df = self._blood_pressure_features(df);     pbar.update(1)\n            df = self._glucose_lipid_features(df);      pbar.update(1)\n            df = self._polynomial_features(df);         pbar.update(1)\n\n        logger.info(f\"  Feature count after engineering: {df.shape[1]}\")\n        return df\n\n    def _cardiovascular_risk(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Framingham-inspired CVD risk proxy.\"\"\"\n        if all(c in df.columns for c in\n               [\"age\", \"systolic_bp\", \"total_cholesterol\", \"hdl_cholesterol\",\n                \"smoker\"]):\n            tc  = df[\"total_cholesterol\"].clip(100, 400)\n            hdl = df[\"hdl_cholesterol\"].clip(10, 120)\n            df[\"cvd_risk_score\"] = (\n                0.04 * df[\"age\"]\n                + 0.01 * df[\"systolic_bp\"]\n                + 0.01 * tc\n                - 0.01 * hdl\n                + 0.3  * df.get(\"smoker\", 0)\n            ).astype(np.float32)\n        return df\n\n    def _metabolic_syndrome_score(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Metabolic syndrome component count (0-5).\"\"\"\n        score = pd.Series(0, index=df.index, dtype=np.float32)\n        if \"waist_cm\" in df.columns:\n            score += (((df[\"gender\"] == 0) & (df[\"waist_cm\"] > 102)) |\n                      ((df[\"gender\"] == 1) & (df[\"waist_cm\"] > 88))).astype(int)\n        if \"fasting_glucose\" in df.columns:\n            score += (df[\"fasting_glucose\"] >= 100).astype(int)\n        if \"systolic_bp\" in df.columns:\n            score += (df[\"systolic_bp\"] >= 130).astype(int)\n        if \"hdl_cholesterol\" in df.columns:\n            score += (((df[\"gender\"] == 0) & (df[\"hdl_cholesterol\"] < 40)) |\n                      ((df[\"gender\"] == 1) & (df[\"hdl_cholesterol\"] < 50))).astype(int)\n        if \"total_cholesterol\" in df.columns and \"hdl_cholesterol\" in df.columns:\n            trig_proxy = df[\"total_cholesterol\"] - df[\"hdl_cholesterol\"]\n            score += (trig_proxy > 150).astype(int)\n        df[\"metabolic_syndrome_score\"] = score\n        return df\n\n    def _lifestyle_composite(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Composite lifestyle risk: higher = worse.\"\"\"\n        score = pd.Series(0.0, index=df.index, dtype=np.float32)\n        if \"smoker\" in df.columns:\n            score += df[\"smoker\"].fillna(0) * 2.0\n        if \"alcohol_drinks\" in df.columns:\n            score += (df[\"alcohol_drinks\"].fillna(0) > 2).astype(float)\n        if \"physically_active\" in df.columns:\n            score -= df[\"physically_active\"].fillna(0) * 1.5\n        if \"sleep_hours\" in df.columns:\n            sh = df[\"sleep_hours\"].fillna(7)\n            score += ((sh < 5) | (sh > 9)).astype(float)\n        if \"stress_level\" in df.columns:\n            score += df[\"stress_level\"].fillna(5) / 10.0\n        if \"depression_phq\" in df.columns:\n            score += df[\"depression_phq\"].fillna(0) / 27.0\n        df[\"lifestyle_risk_composite\"] = score.astype(np.float32)\n        return df\n\n    def _sleep_stress_interaction(self, df: pd.DataFrame) -> pd.DataFrame:\n        if \"sleep_hours\" in df.columns and \"stress_level\" in df.columns:\n            df[\"sleep_stress_ratio\"] = (\n                df[\"stress_level\"].fillna(5) /\n                df[\"sleep_hours\"].fillna(7).replace(0, 1)\n            ).astype(np.float32)\n        return df\n\n    def _bmi_age_interaction(self, df: pd.DataFrame) -> pd.DataFrame:\n        if \"bmi\" in df.columns and \"age\" in df.columns:\n            df[\"bmi_age_product\"] = (df[\"bmi\"] * df[\"age\"] / 100).astype(np.float32)\n            df[\"bmi_sq\"] = (df[\"bmi\"] ** 2 / 1000).astype(np.float32)\n        return df\n\n    def _blood_pressure_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        if \"systolic_bp\" in df.columns and \"diastolic_bp\" in df.columns:\n            df[\"pulse_pressure\"] = (\n                df[\"systolic_bp\"] - df[\"diastolic_bp\"]).astype(np.float32)\n            df[\"mean_arterial_pressure\"] = (\n                df[\"diastolic_bp\"] + df[\"pulse_pressure\"] / 3).astype(np.float32)\n            df[\"hypertension_stage\"] = pd.cut(\n                df[\"systolic_bp\"],\n                bins=[0, 120, 130, 140, 180, 999],\n                labels=[0, 1, 2, 3, 4]\n            ).astype(float).fillna(0).astype(np.float32)\n        return df\n\n    def _glucose_lipid_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        if \"fasting_glucose\" in df.columns:\n            df[\"prediabetes_flag\"] = (\n                (df[\"fasting_glucose\"] >= 100) & (df[\"fasting_glucose\"] < 126)\n            ).astype(np.int8)\n            df[\"diabetes_flag\"] = (df[\"fasting_glucose\"] >= 126).astype(np.int8)\n        if \"hba1c\" in df.columns:\n            df[\"hba1c_risk\"] = pd.cut(\n                df[\"hba1c\"], bins=[0, 5.7, 6.5, 100],\n                labels=[0, 1, 2]\n            ).astype(float).fillna(0).astype(np.float32)\n        if all(c in df.columns for c in [\"total_cholesterol\", \"hdl_cholesterol\"]):\n            df[\"cholesterol_ratio\"] = (\n                df[\"total_cholesterol\"] / df[\"hdl_cholesterol\"].replace(0, 1)\n            ).astype(np.float32)\n        return df\n\n    def _polynomial_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Interaction terms between top clinical features.\"\"\"\n        pairs = [\n            (\"bmi\", \"age\"), (\"systolic_bp\", \"age\"),\n            (\"fasting_glucose\", \"bmi\"), (\"total_cholesterol\", \"age\"),\n        ]\n        for a, b in pairs:\n            if a in df.columns and b in df.columns:\n                df[f\"{a}x{b}\"] = (df[a] * df[b] / 1000).astype(np.float32)\n        return df\n\n    def select_features(self, X: pd.DataFrame, y: pd.Series,\n                        n_features: int = 40) -> pd.DataFrame:\n        \"\"\"Mutual information + RandomForest importance–based selection.\"\"\"\n        logger.info(f\"  Selecting top {n_features} features ...\")\n        n_features = min(n_features, X.shape[1])\n\n        # Mutual information\n        mi = mutual_info_classif(X.values, y.values, random_state=RANDOM_STATE)\n        mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n\n        # RandomForest quick importance\n        rf = RandomForestClassifier(n_estimators=50, max_depth=5,\n                                    random_state=RANDOM_STATE, n_jobs=-1)\n        rf.fit(X.values, y.values)\n        fi_series = pd.Series(rf.feature_importances_,\n                              index=X.columns).sort_values(ascending=False)\n        force_cleanup(rf)\n\n        # Combine rankings\n        rank_mi = mi_series.rank(ascending=False)\n        rank_fi = fi_series.rank(ascending=False)\n        combined = (rank_mi + rank_fi).sort_values()\n        self.selected_features = combined.index[:n_features].tolist()\n        self.feature_importances_ = fi_series\n\n        self._plot_feature_importance(fi_series.head(25))\n        return X[self.selected_features]\n\n    def _plot_feature_importance(self, fi: pd.Series) -> None:\n        fig, ax = plt.subplots(figsize=(10, 8))\n        fi.sort_values().plot.barh(ax=ax, color=\"#42A5F5\")\n        ax.set_title(\"RandomForest Feature Importance (Top 25)\", fontsize=12,\n                     fontweight=\"bold\")\n        ax.set_xlabel(\"Importance\")\n        plt.tight_layout()\n        path = PLOTS_DIR / \"feature_importance.png\"\n        plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n        plt.show()\n        plt.close()\n        logger.info(f\"  Saved: {path}\")\n\n    def compute_pca(self, X: np.ndarray,\n                    n_components: int = 10) -> np.ndarray:\n        \"\"\"Append PCA components to feature matrix.\"\"\"\n        n_components = min(n_components, X.shape[1], X.shape[0] - 1)\n        self.pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n        pca_feats = self.pca.fit_transform(X)\n        self._plot_pca_variance()\n        return np.hstack([X, pca_feats])\n\n    def _plot_pca_variance(self) -> None:\n        if self.pca is None:\n            return\n        evr = self.pca.explained_variance_ratio_\n        cumulative = np.cumsum(evr)\n        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n        axes[0].bar(range(1, len(evr) + 1), evr, color=\"#66BB6A\")\n        axes[0].set_title(\"Explained Variance per PC\")\n        axes[0].set_xlabel(\"Principal Component\")\n        axes[0].set_ylabel(\"Variance Ratio\")\n\n        axes[1].plot(range(1, len(cumulative) + 1), cumulative,\n                     \"b-o\", linewidth=2)\n        axes[1].axhline(0.9, color=\"red\", linestyle=\"--\", label=\"90% threshold\")\n        axes[1].set_title(\"Cumulative Explained Variance\")\n        axes[1].set_xlabel(\"Principal Component\")\n        axes[1].set_ylabel(\"Cumulative Variance Ratio\")\n        axes[1].legend()\n\n        fig.suptitle(\"PCA Analysis\", fontsize=13, fontweight=\"bold\")\n        plt.tight_layout()\n        path = PLOTS_DIR / \"feature_pca_variance.png\"\n        plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n        plt.show()\n        plt.close()\n        logger.info(f\"  Saved: {path}\")\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 5 — DATA SPLITTING  (Train/Val/Test/Holdout)\n# ══════════════════════════════════════════════════════════════════════════════\n\ndef verify_data_splits(splits: dict) -> bool:\n    \"\"\"\n    Validate four-way split:\n    - Correct proportions\n    - No index overlap (no data leakage)\n    - Class balance reasonable\n    \"\"\"\n    logger.info(\"Verifying data splits ...\")\n    total = sum(len(v[0]) for v in splits.values())\n    all_ok = True\n\n    # Index overlap check\n    index_sets = {k: set(range(offset, offset + len(v[0])))\n                  for k, v in splits.items()\n                  for offset in [0]}\n    # Re-check via original indices stored alongside\n    names = list(splits.keys())\n\n    for i in range(len(names)):\n        for j in range(i + 1, len(names)):\n            n1, n2 = names[i], names[j]\n            Xi, _ = splits[n1]\n            Xj, _ = splits[n2]\n            # Check shapes don't share rows (robust for numpy arrays)\n            if Xi.shape[0] + Xj.shape[0] > total:\n                logger.error(f\"  LEAK detected between {n1} and {n2}!\")\n                all_ok = False\n\n    # Proportion check\n    expected = SPLIT_CONFIG\n    for name, (X, y) in splits.items():\n        frac = len(X) / total\n        exp_frac = expected.get(name, 0)\n        diff = abs(frac - exp_frac)\n        status = \"\" if diff < 0.03 else \"✗\"\n        logger.info(f\"  {status} {name:8s}: n={len(X):5d}  \"\n                    f\"frac={frac:.3f}  expected≈{exp_frac:.2f}  \"\n                    f\"pos_rate={y.mean():.3f}\")\n        if diff >= 0.05:\n            all_ok = False\n\n    return all_ok\n\n\ndef four_way_split(X: np.ndarray, y: np.ndarray):\n    \"\"\"\n    Strict four-way stratified split:\n      Holdout (30%) → set aside immediately, never touched again\n      Remaining 70% → Train(40/70≈57%), Val(15/70≈21%), Test(15/70≈21%)\n    \"\"\"\n    logger.info(\"Creating four-way stratified split ...\")\n    total = len(X)\n\n    # Step 1: Carve out holdout (30%)\n    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.30,\n                                  random_state=RANDOM_STATE)\n    dev_idx, holdout_idx = next(sss1.split(X, y))\n\n    X_dev,     y_dev     = X[dev_idx],     y[dev_idx]\n    X_holdout, y_holdout = X[holdout_idx], y[holdout_idx]\n\n    # Step 2: From remaining 70%, carve test (15/70 ≈ 0.2143)\n    test_frac = SPLIT_CONFIG[\"test\"] / (1 - SPLIT_CONFIG[\"holdout\"])\n    sss2 = StratifiedShuffleSplit(n_splits=1,\n                                  test_size=test_frac,\n                                  random_state=RANDOM_STATE + 1)\n    trainval_idx, test_idx = next(sss2.split(X_dev, y_dev))\n\n    X_trainval, y_trainval = X_dev[trainval_idx], y_dev[trainval_idx]\n    X_test,     y_test     = X_dev[test_idx],     y_dev[test_idx]\n\n    # Step 3: From trainval, carve val (15/55 ≈ 0.2727)\n    val_frac = SPLIT_CONFIG[\"val\"] / (SPLIT_CONFIG[\"train\"] + SPLIT_CONFIG[\"val\"])\n    sss3 = StratifiedShuffleSplit(n_splits=1,\n                                  test_size=val_frac,\n                                  random_state=RANDOM_STATE + 2)\n    train_idx, val_idx = next(sss3.split(X_trainval, y_trainval))\n\n    X_train, y_train = X_trainval[train_idx], y_trainval[train_idx]\n    X_val,   y_val   = X_trainval[val_idx],   y_trainval[val_idx]\n\n    splits = {\n        \"train\":   (X_train,   y_train),\n        \"val\":     (X_val,     y_val),\n        \"test\":    (X_test,    y_test),\n        \"holdout\": (X_holdout, y_holdout),\n    }\n\n    verify_data_splits(splits)\n    return splits\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 6 — OUTLIER DETECTION & SAMPLE WEIGHTS\n# ══════════════════════════════════════════════════════════════════════════════\n\ndef compute_sample_weights(X_train: np.ndarray,\n                           y_train: np.ndarray) -> np.ndarray:\n    \"\"\"\n    IsolationForest outlier scores → adjust training sample weights.\n    Outlier samples get reduced weight; typical samples get unit weight.\n    \"\"\"\n    logger.info(\"Computing outlier-adjusted sample weights ...\")\n    iso = IsolationForest(n_estimators=100, contamination=0.05,\n                          random_state=RANDOM_STATE, n_jobs=-1)\n    iso.fit(X_train)\n    scores = iso.score_samples(X_train)       # More negative = more anomalous\n    # Normalise to [0.3, 1.0]  — outliers get min weight 0.3\n    normalised = (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)\n    weights = 0.3 + 0.7 * normalised\n    force_cleanup(iso)\n    logger.info(f\"  Weight range: [{weights.min():.3f}, {weights.max():.3f}]\")\n    return weights.astype(np.float32)\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 7 — NEURAL NETWORK WITH ATTENTION\n# ══════════════════════════════════════════════════════════════════════════════\n\ndef build_attention_nn(input_dim: int) -> \"tf.keras.Model\":\n    \"\"\"\n    Hybrid Dense + Multi-Head Self-Attention network.\n    Anti-overfitting: L2, Dropout(40%), BatchNorm, GaussianNoise.\n    \"\"\"\n    inp = Input(shape=(input_dim,), name=\"input\")\n\n    # Input noise augmentation\n    x = GaussianNoise(0.05)(inp)\n\n    # Block 1\n    x = Dense(128, activation=\"relu\", kernel_regularizer=l2(1e-4))(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n\n    # Block 2\n    x = Dense(64, activation=\"relu\", kernel_regularizer=l2(1e-4))(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.35)(x)\n\n    # Attention block: reshape to seq, apply attention, flatten\n    x_seq = Reshape((1, 64))(x)\n    attn_out, _ = MultiHeadAttention(num_heads=4, key_dim=16)(\n        x_seq, x_seq, return_attention_scores=True)\n    attn_out = LayerNormalization()(attn_out + x_seq)  # Residual\n    x = GlobalAveragePooling1D()(attn_out)\n\n    # Block 3\n    x = Dense(32, activation=\"relu\", kernel_regularizer=l2(1e-4))(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n\n    out = Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n\n    model = Model(inputs=inp, outputs=out, name=\"AttentionNet\")\n    model.compile(optimizer=Adam(learning_rate=1e-3),\n                  loss=\"binary_crossentropy\",\n                  metrics=[\"accuracy\",\n                            tf.keras.metrics.AUC(name=\"auc\"),\n                            tf.keras.metrics.Precision(name=\"precision\"),\n                            tf.keras.metrics.Recall(name=\"recall\")])\n    return model\n\n\ndef train_neural_network(X_train, y_train, X_val, y_val,\n                          sample_weights=None):\n    \"\"\"Train AttentionNet with callbacks; return model + history.\"\"\"\n    if not TF_AVAILABLE:\n        return None, None\n\n    logger.info(\"Training AttentionNet ...\")\n    model = build_attention_nn(X_train.shape[1])\n    model.summary(print_fn=lambda s: logger.info(\"  \" + s))\n\n    checkpoint_path = str(MODELS_DIR / \"attention_nn_best.keras\")   # changed .h5 to .keras\n    callbacks = [\n        EarlyStopping(monitor=\"val_accuracy\", patience=NN_PATIENCE,\n                      restore_best_weights=True, min_delta=1e-4, mode=\"max\"),\n        ModelCheckpoint(checkpoint_path, monitor=\"val_accuracy\",\n                        save_best_only=True, mode=\"max\", verbose=0),\n        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5,\n                          min_lr=1e-6, verbose=0),\n    ]\n\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=NN_EPOCHS,\n        batch_size=BATCH_SIZE_NN,\n        callbacks=callbacks,\n        sample_weight=sample_weights,\n        verbose=0,\n        shuffle=True,\n    )\n    logger.info(f\"  Best val_accuracy: {max(history.history['val_accuracy']):.4f}\")\n    return model, history\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 8 — CLASSICAL ML MODELS\n# ══════════════════════════════════════════════════════════════════════════════\n\ndef build_classical_models(n_features: int) -> dict:\n    \"\"\"Return dictionary of instantiated classical models.\"\"\"\n    models = {\n        \"GradientBoosting\": GradientBoostingClassifier(\n            n_estimators=200, max_depth=4, learning_rate=0.05,\n            subsample=0.8, max_features=\"sqrt\",\n            validation_fraction=0.1, n_iter_no_change=15,\n            random_state=RANDOM_STATE),\n        \"RandomForest\": RandomForestClassifier(\n            n_estimators=200, max_depth=8, min_samples_leaf=5,\n            max_features=\"sqrt\", class_weight=\"balanced\",\n            random_state=RANDOM_STATE, n_jobs=-1),\n        \"LogisticRegression\": LogisticRegression(\n            C=0.5, max_iter=1000, solver=\"saga\",\n            class_weight=\"balanced\", random_state=RANDOM_STATE),\n    }\n    if XGB_AVAILABLE:\n        models[\"XGBoost\"] = xgb.XGBClassifier(\n            n_estimators=200, max_depth=4, learning_rate=0.05,\n            subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1,\n            reg_lambda=1.0, use_label_encoder=False,\n            eval_metric=\"logloss\", random_state=RANDOM_STATE,\n            n_jobs=-1)\n    if LGB_AVAILABLE:\n        models[\"LightGBM\"] = lgb.LGBMClassifier(\n            n_estimators=200, max_depth=4, learning_rate=0.05,\n            subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1,\n            reg_lambda=1.0, class_weight=\"balanced\",\n            random_state=RANDOM_STATE, n_jobs=-1, verbose=-1)\n    return models\n\n\ndef train_classical_models(models: dict, X_train, y_train,\n                             X_val, y_val, sample_weights=None):\n    \"\"\"Train each classical model; return results dict.\"\"\"\n    results = {}\n    for name, model in tqdm(models.items(), desc=\"Training classical models\"):\n        try:\n            t0 = time.time()\n            if sample_weights is not None and hasattr(model, \"fit\"):\n                try:\n                    model.fit(X_train, y_train, sample_weight=sample_weights)\n                except TypeError:\n                    model.fit(X_train, y_train)\n            else:\n                model.fit(X_train, y_train)\n            elapsed = time.time() - t0\n            val_pred  = model.predict(X_val)\n            val_proba = model.predict_proba(X_val)[:, 1]\n            val_acc   = accuracy_score(y_val, val_pred)\n            val_auc   = roc_auc_score(y_val, val_proba)\n            results[name] = {\n                \"model\":    model,\n                \"val_acc\":  val_acc,\n                \"val_auc\":  val_auc,\n                \"time_s\":   elapsed,\n            }\n            logger.info(f\"  {name:20s}  val_acc={val_acc:.4f}  \"\n                        f\"val_auc={val_auc:.4f}  t={elapsed:.1f}s\")\n        except Exception as exc:\n            logger.error(f\"  {name} failed: {exc}\")\n        check_memory_limit()\n        gc.collect()\n    return results\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 9 — HYPERPARAMETER TUNING\n# ══════════════════════════════════════════════════════════════════════════════\n\ndef tune_hyperparameters(X_train, y_train, X_val, y_val,\n                          best_model_name: str) -> object:\n    \"\"\"\n    Grid search over a focused hyperparameter grid for the best classical\n    model. Uses validation-set evaluation (no re-fitting on val).\n    \"\"\"\n    from sklearn.model_selection import ParameterGrid\n\n    logger.info(f\"Hyperparameter tuning for {best_model_name} ...\")\n\n    param_grids = {\n        \"GradientBoosting\": {\n            \"n_estimators\": [200, 300],\n            \"max_depth\":    [3, 4, 5],\n            \"learning_rate\":[0.03, 0.05, 0.1],\n            \"subsample\":    [0.7, 0.9],\n        },\n        \"RandomForest\": {\n            \"n_estimators\": [200, 300],\n            \"max_depth\":    [6, 8, 10],\n            \"min_samples_leaf\": [3, 5],\n        },\n        \"XGBoost\": {\n            \"n_estimators\": [200, 300],\n            \"max_depth\":    [3, 4],\n            \"learning_rate\":[0.03, 0.07],\n            \"subsample\":    [0.7, 0.9],\n        },\n        \"LightGBM\": {\n            \"n_estimators\": [200, 300],\n            \"max_depth\":    [4, 6],\n            \"learning_rate\":[0.03, 0.07],\n        },\n    }\n\n    grid = param_grids.get(best_model_name, {})\n    if not grid:\n        logger.info(\"  No tuning grid defined — returning as-is.\")\n        return None\n\n    all_params = list(ParameterGrid(grid))\n    best_acc, best_params, best_model = 0, {}, None\n\n    with tqdm(total=len(all_params), desc=\"  Hyperparameter grid\") as pbar:\n        for params in all_params:\n            try:\n                if best_model_name == \"GradientBoosting\":\n                    m = GradientBoostingClassifier(**params,\n                            random_state=RANDOM_STATE)\n                elif best_model_name == \"RandomForest\":\n                    m = RandomForestClassifier(**params,\n                            class_weight=\"balanced\",\n                            random_state=RANDOM_STATE, n_jobs=-1)\n                elif best_model_name == \"XGBoost\" and XGB_AVAILABLE:\n                    m = xgb.XGBClassifier(**params,\n                            use_label_encoder=False,\n                            eval_metric=\"logloss\",\n                            random_state=RANDOM_STATE, n_jobs=-1)\n                elif best_model_name == \"LightGBM\" and LGB_AVAILABLE:\n                    m = lgb.LGBMClassifier(**params,\n                            class_weight=\"balanced\",\n                            random_state=RANDOM_STATE, n_jobs=-1,\n                            verbose=-1)\n                else:\n                    pbar.update(1)\n                    continue\n\n                m.fit(X_train, y_train)\n                acc = accuracy_score(y_val, m.predict(X_val))\n                if acc > best_acc:\n                    best_acc = acc\n                    best_params = params\n                    best_model = m\n                force_cleanup(m)\n            except Exception as exc:\n                logger.warning(f\"    Tuning step failed: {exc}\")\n            pbar.update(1)\n            check_memory_limit()\n\n    logger.info(f\"  Best params: {best_params}  val_acc={best_acc:.4f}\")\n    return best_model\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 10 — ENSEMBLE / STACKING\n# ══════════════════════════════════════════════════════════════════════════════\n\ndef build_stacking_ensemble(base_results: dict,\n                             X_train, y_train,\n                             X_val, y_val) -> object:\n    \"\"\"\n    Stacking ensemble: top 3 base classifiers → LogisticRegression meta.\n    Also builds a soft-voting ensemble for comparison.\n    \"\"\"\n    logger.info(\"Building stacking ensemble ...\")\n\n    # Select top-3 by val_acc\n    sorted_models = sorted(base_results.items(),\n                           key=lambda x: x[1][\"val_acc\"], reverse=True)[:3]\n    estimators = [(n, r[\"model\"]) for n, r in sorted_models]\n\n    if len(estimators) < 2:\n        logger.warning(\"  Not enough base models for stacking.\")\n        return None\n\n    # Stacking\n    stack = StackingClassifier(\n        estimators=estimators,\n        final_estimator=LogisticRegression(C=0.5, max_iter=500,\n                                            random_state=RANDOM_STATE),\n        cv=StratifiedKFold(n_splits=3, shuffle=True,\n                           random_state=RANDOM_STATE),\n        n_jobs=-1,\n        passthrough=False,\n    )\n    stack.fit(X_train, y_train)\n    val_acc = accuracy_score(y_val, stack.predict(X_val))\n    logger.info(f\"  Stacking val_acc={val_acc:.4f}\")\n    gc.collect()\n    return stack, val_acc\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 11 — EVALUATION METRICS\n# ══════════════════════════════════════════════════════════════════════════════\n\ndef evaluate_model(model, X: np.ndarray, y: np.ndarray,\n                   split_name: str, nn_model=None) -> dict:\n    \"\"\"Full evaluation on any split.\"\"\"\n    # Predictions\n    if nn_model is not None and TF_AVAILABLE:\n        proba_nn = nn_model.predict(X, verbose=0).flatten()\n    else:\n        proba_nn = None\n\n    proba = model.predict_proba(X)[:, 1]\n    pred  = model.predict(X)\n\n    if proba_nn is not None:\n        proba_ensemble = 0.6 * proba + 0.4 * proba_nn\n        pred_ensemble  = (proba_ensemble >= 0.5).astype(int)\n    else:\n        proba_ensemble = proba\n        pred_ensemble  = pred\n\n    metrics = {\n        \"split\":      split_name,\n        \"accuracy\":   accuracy_score(y, pred_ensemble),\n        \"precision\":  precision_score(y, pred_ensemble, zero_division=0),\n        \"recall\":     recall_score(y, pred_ensemble, zero_division=0),\n        \"f1\":         f1_score(y, pred_ensemble, zero_division=0),\n        \"roc_auc\":    roc_auc_score(y, proba_ensemble),\n        \"avg_precision\": average_precision_score(y, proba_ensemble),\n        \"n_samples\":  len(y),\n        \"proba\":      proba_ensemble,\n        \"pred\":       pred_ensemble,\n    }\n    logger.info(f\"  {split_name:8s}  acc={metrics['accuracy']:.4f}  \"\n                f\"auc={metrics['roc_auc']:.4f}  f1={metrics['f1']:.4f}  \"\n                f\"prec={metrics['precision']:.4f}  rec={metrics['recall']:.4f}\")\n    return metrics\n\n\ndef detect_overfitting(train_metrics: dict,\n                       val_metrics:   dict,\n                       test_metrics:  dict,\n                       holdout_metrics: dict) -> str:\n    \"\"\"\n    Rule-based overfitting detection.\n    Returns human-readable verdict.\n    \"\"\"\n    train_acc   = train_metrics[\"accuracy\"]\n    val_acc     = val_metrics[\"accuracy\"]\n    test_acc    = test_metrics[\"accuracy\"]\n    holdout_acc = holdout_metrics[\"accuracy\"]\n\n    gap_tv  = train_acc - val_acc\n    gap_th  = train_acc - holdout_acc\n    gen_gap = train_acc - np.mean([val_acc, test_acc, holdout_acc])\n\n    if gen_gap > 0.15:\n        verdict = \"⚠ SEVERE OVERFITTING — reduce complexity or add regularisation\"\n    elif gen_gap > 0.07:\n        verdict = \"⚠ MODERATE OVERFITTING — consider more dropout / early stopping\"\n    elif gen_gap < 0.01 and holdout_acc < 0.6:\n        verdict = \"⚠ UNDERFITTING — model may need more capacity or features\"\n    else:\n        verdict = \" GOOD GENERALISATION — train/holdout gap within healthy range\"\n\n    logger.info(f\"  Overfitting check: {verdict}\")\n    logger.info(f\"    Train-Val gap:     {gap_tv:.4f}\")\n    logger.info(f\"    Train-Holdout gap: {gap_th:.4f}\")\n    logger.info(f\"    Generalisation gap:{gen_gap:.4f}\")\n    return verdict\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 12 — CROSS VALIDATION\n# ══════════════════════════════════════════════════════════════════════════════\n\ndef run_cross_validation(model, X: np.ndarray, y: np.ndarray,\n                          n_splits: int = 5) -> dict:\n    \"\"\"5-fold stratified CV on training+val data.\"\"\"\n    logger.info(f\"5-fold cross-validation ...\")\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True,\n                          random_state=RANDOM_STATE)\n\n    metrics = {\"accuracy\": [], \"roc_auc\": [], \"f1\": []}\n    with tqdm(total=n_splits, desc=\"  CV folds\") as pbar:\n        for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y)):\n            X_tr, X_va = X[tr_idx], X[va_idx]\n            y_tr, y_va = y[tr_idx], y[va_idx]\n            try:\n                import copy\n                m = copy.deepcopy(model)\n                m.fit(X_tr, y_tr)\n                pred  = m.predict(X_va)\n                proba = m.predict_proba(X_va)[:, 1]\n                metrics[\"accuracy\"].append(accuracy_score(y_va, pred))\n                metrics[\"roc_auc\"].append(roc_auc_score(y_va, proba))\n                metrics[\"f1\"].append(f1_score(y_va, pred, zero_division=0))\n                force_cleanup(m)\n            except Exception as exc:\n                logger.warning(f\"  CV fold {fold+1} failed: {exc}\")\n            pbar.update(1)\n            gc.collect()\n\n    summary = {k: {\"mean\": np.mean(v), \"std\": np.std(v)}\n               for k, v in metrics.items() if v}\n    for k, s in summary.items():\n        logger.info(f\"  CV {k}: {s['mean']:.4f} ± {s['std']:.4f}\")\n    return summary\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 13 — VISUALISATION\n# ══════════════════════════════════════════════════════════════════════════════\n\nclass Visualiser:\n    \"\"\"All evaluation plots — each saved + shown.\"\"\"\n\n    @staticmethod\n    def save_show(path: Path) -> None:\n        plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n        plt.show()\n        plt.close()\n        logger.info(f\"  Saved: {path}\")\n\n    def plot_roc_curves(self, all_metrics: dict) -> None:\n        \"\"\"ROC curves for all splits on the same axes.\"\"\"\n        fig, ax = plt.subplots(figsize=(8, 6))\n        for split, m in all_metrics.items():\n            if \"proba\" not in m:\n                continue\n            fpr, tpr, _ = roc_curve(m[\"y_true\"], m[\"proba\"])\n            ax.plot(fpr, tpr, linewidth=2, color=PALETTE.get(split, \"gray\"),\n                    label=f\"{split} (AUC={m['roc_auc']:.3f})\")\n        ax.plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"Random\")\n        ax.set_xlabel(\"False Positive Rate\"); ax.set_ylabel(\"True Positive Rate\")\n        ax.set_title(\"ROC Curves — All Splits\", fontsize=13, fontweight=\"bold\")\n        ax.legend(loc=\"lower right\")\n        ax.grid(alpha=0.3)\n        self.save_show(PLOTS_DIR / \"eval_roc_curves.png\")\n\n    def plot_precision_recall(self, all_metrics: dict) -> None:\n        fig, ax = plt.subplots(figsize=(8, 6))\n        for split, m in all_metrics.items():\n            if \"proba\" not in m:\n                continue\n            prec, rec, _ = precision_recall_curve(m[\"y_true\"], m[\"proba\"])\n            ax.plot(rec, prec, linewidth=2, color=PALETTE.get(split, \"gray\"),\n                    label=f\"{split} (AP={m['avg_precision']:.3f})\")\n        ax.set_xlabel(\"Recall\"); ax.set_ylabel(\"Precision\")\n        ax.set_title(\"Precision-Recall Curves\", fontsize=13, fontweight=\"bold\")\n        ax.legend(loc=\"upper right\")\n        ax.grid(alpha=0.3)\n        self.save_show(PLOTS_DIR / \"eval_pr_curves.png\")\n\n    def plot_confusion_matrices(self, all_metrics: dict) -> None:\n        n = len(all_metrics)\n        fig, axes = plt.subplots(1, n, figsize=(5 * n, 4))\n        if n == 1:\n            axes = [axes]\n        for ax, (split, m) in zip(axes, all_metrics.items()):\n            cm = confusion_matrix(m[\"y_true\"], m[\"pred\"])\n            sns.heatmap(cm, annot=True, fmt=\"d\", ax=ax,\n                        cmap=\"Blues\", linewidths=0.5,\n                        xticklabels=[\"Low Risk\", \"High Risk\"],\n                        yticklabels=[\"Low Risk\", \"High Risk\"])\n            ax.set_title(f\"{split}\\nacc={m['accuracy']:.3f}\",\n                         fontsize=11, fontweight=\"bold\")\n            ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n        fig.suptitle(\"Confusion Matrices — All Splits\", fontsize=13,\n                     fontweight=\"bold\")\n        plt.tight_layout()\n        self.save_show(PLOTS_DIR / \"eval_confusion_matrices.png\")\n\n    def plot_metrics_bar(self, all_metrics: dict) -> None:\n        metric_names = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]\n        splits = list(all_metrics.keys())\n        x = np.arange(len(metric_names))\n        width = 0.8 / len(splits)\n\n        fig, ax = plt.subplots(figsize=(14, 6))\n        for i, split in enumerate(splits):\n            values = [all_metrics[split].get(m, 0) for m in metric_names]\n            offset = (i - len(splits) / 2 + 0.5) * width\n            bars = ax.bar(x + offset, values, width,\n                          label=split, color=PALETTE.get(split, \"#999\"))\n            for bar, v in zip(bars, values):\n                if v > 0:\n                    ax.text(bar.get_x() + bar.get_width() / 2,\n                            bar.get_height() + 0.005,\n                            f\"{v:.3f}\", ha=\"center\", va=\"bottom\",\n                            fontsize=7, rotation=45)\n\n        ax.set_xticks(x)\n        ax.set_xticklabels(metric_names, fontsize=11)\n        ax.set_ylim(0, 1.15)\n        ax.set_ylabel(\"Score\")\n        ax.set_title(\"Evaluation Metrics — All Splits\", fontsize=13,\n                     fontweight=\"bold\")\n        ax.legend(loc=\"upper right\")\n        ax.axhline(0.9, color=\"green\", linestyle=\"--\", linewidth=1,\n                   label=\"0.90 target\")\n        ax.grid(axis=\"y\", alpha=0.3)\n        plt.tight_layout()\n        self.save_show(PLOTS_DIR / \"eval_metrics_bar.png\")\n\n    def plot_generalization_gap(self, all_metrics: dict) -> None:\n        \"\"\"Show train vs val/test/holdout gap for overfitting detection.\"\"\"\n        train_acc = all_metrics.get(\"train\", {}).get(\"accuracy\", 0)\n        splits = [\"val\", \"test\", \"holdout\"]\n        gaps = [train_acc - all_metrics.get(s, {}).get(\"accuracy\", train_acc)\n                for s in splits]\n        accs = [all_metrics.get(s, {}).get(\"accuracy\", 0) for s in splits]\n\n        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n        # Left: gap bars\n        colors = [\"#4CAF50\" if g < 0.05 else \"#FF9800\" if g < 0.10 else \"#F44336\"\n                  for g in gaps]\n        axes[0].bar(splits, gaps, color=colors)\n        axes[0].axhline(0.05, color=\"orange\", linestyle=\"--\", label=\"5% warning\")\n        axes[0].axhline(0.10, color=\"red\",    linestyle=\"--\", label=\"10% critical\")\n        axes[0].set_title(\"Generalisation Gap (Train − Split Accuracy)\",\n                          fontsize=12, fontweight=\"bold\")\n        axes[0].set_ylabel(\"Accuracy Gap\")\n        axes[0].legend()\n        for i, (s, g) in enumerate(zip(splits, gaps)):\n            axes[0].text(i, g + 0.002, f\"{g:.4f}\", ha=\"center\", fontsize=10)\n\n        # Right: accuracy across all splits\n        all_splits = [\"train\"] + splits\n        all_accs   = [all_metrics.get(s, {}).get(\"accuracy\", 0) for s in all_splits]\n        bar_colors = [PALETTE.get(s, \"#999\") for s in all_splits]\n        bars = axes[1].bar(all_splits, all_accs, color=bar_colors)\n        axes[1].set_title(\"Accuracy — All Splits\", fontsize=12, fontweight=\"bold\")\n        axes[1].set_ylabel(\"Accuracy\")\n        axes[1].set_ylim(0, 1.1)\n        axes[1].axhline(0.9, color=\"green\", linestyle=\"--\", label=\"0.90 line\")\n        axes[1].legend()\n        for bar, acc in zip(bars, all_accs):\n            axes[1].text(bar.get_x() + bar.get_width() / 2,\n                         bar.get_height() + 0.01,\n                         f\"{acc:.3f}\", ha=\"center\", fontsize=10)\n\n        fig.suptitle(\"Overfitting / Generalisation Analysis\", fontsize=14,\n                     fontweight=\"bold\")\n        plt.tight_layout()\n        self.save_show(PLOTS_DIR / \"eval_generalization_gap.png\")\n\n    def plot_nn_history(self, history) -> None:\n        if history is None:\n            return\n        h = history.history\n        epochs = range(1, len(h[\"accuracy\"]) + 1)\n        best_epoch = np.argmax(h[\"val_accuracy\"]) + 1\n\n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n        pairs = [(\"accuracy\", \"val_accuracy\", \"Accuracy\"),\n                 (\"loss\",     \"val_loss\",     \"Loss\"),\n                 (\"auc\",      \"val_auc\",      \"AUC\"),\n                 (\"precision\",\"val_precision\",\"Precision\")]\n\n        for ax, (tr_key, va_key, title) in zip(axes.flatten(), pairs):\n            if tr_key not in h:\n                ax.set_visible(False)\n                continue\n            ax.plot(epochs, h[tr_key], color=PALETTE[\"train\"],\n                    linewidth=2, label=\"Train\")\n            ax.plot(epochs, h[va_key], color=PALETTE[\"val\"],\n                    linewidth=2, label=\"Validation\")\n            ax.axvline(best_epoch, color=\"red\", linestyle=\"--\",\n                       label=f\"Best epoch ({best_epoch})\")\n            best_val = max(h[va_key]) if \"loss\" not in va_key else min(h[va_key])\n            ax.scatter([best_epoch], [best_val], color=\"red\", s=60, zorder=5)\n            ax.set_title(title, fontsize=11, fontweight=\"bold\")\n            ax.set_xlabel(\"Epoch\"); ax.set_ylabel(title)\n            ax.legend(fontsize=9); ax.grid(alpha=0.3)\n\n        fig.suptitle(\"Neural Network Training History\", fontsize=14,\n                     fontweight=\"bold\")\n        plt.tight_layout()\n        self.save_show(PLOTS_DIR / \"nn_training_history.png\")\n\n    def plot_feature_shap_proxy(self, model, X: np.ndarray,\n                                 feature_names: list) -> None:\n        \"\"\"Permutation importance as SHAP proxy on holdout set.\"\"\"\n        logger.info(\"  Computing permutation importance (SHAP proxy) ...\")\n        try:\n            result = permutation_importance(\n                model, X[:500], None,\n                # We need y for permutation importance\n                n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)\n        except Exception:\n            return\n\n    def plot_permutation_importance(self, model, X: np.ndarray,\n                                     y: np.ndarray, feature_names: list,\n                                     split_name: str = \"holdout\") -> None:\n        logger.info(f\"  Permutation importance on {split_name} ...\")\n        try:\n            n = min(500, len(X))\n            result = permutation_importance(\n                model, X[:n], y[:n],\n                n_repeats=8, random_state=RANDOM_STATE, n_jobs=-1)\n            fi = pd.Series(result.importances_mean,\n                           index=feature_names[:X.shape[1]]).sort_values(ascending=False).head(20)\n\n            fig, ax = plt.subplots(figsize=(10, 7))\n            fi.sort_values().plot.barh(ax=ax, color=\"#AB47BC\")\n            ax.set_title(f\"Permutation Importance ({split_name})\",\n                         fontsize=12, fontweight=\"bold\")\n            ax.set_xlabel(\"Mean Decrease in Accuracy\")\n            plt.tight_layout()\n            self.save_show(PLOTS_DIR / f\"eval_perm_importance_{split_name}.png\")\n        except Exception as exc:\n            logger.warning(f\"  Permutation importance failed: {exc}\")\n\n    def plot_holdout_deep_dive(self, holdout_metrics: dict) -> None:\n        \"\"\"Detailed analysis of holdout (truly unseen) set.\"\"\"\n        m = holdout_metrics\n        proba = m[\"proba\"]\n        y_true = m[\"y_true\"]\n        pred   = m[\"pred\"]\n\n        fig = plt.figure(figsize=(16, 12))\n        gs  = gridspec.GridSpec(2, 3, figure=fig)\n\n        # 1. Probability distribution\n        ax1 = fig.add_subplot(gs[0, 0])\n        for cls, color, label in [(0, PALETTE[\"neg\"], \"Low Risk\"),\n                                   (1, PALETTE[\"pos\"], \"High Risk\")]:\n            ax1.hist(proba[y_true == cls], bins=40, alpha=0.7,\n                     color=color, label=label, density=True)\n        ax1.axvline(0.5, color=\"black\", linestyle=\"--\", linewidth=1.5)\n        ax1.set_title(\"Predicted Probability Distribution\")\n        ax1.set_xlabel(\"P(High Risk)\"); ax1.legend()\n\n        # 2. Calibration curve\n        ax2 = fig.add_subplot(gs[0, 1])\n        from sklearn.calibration import calibration_curve\n        try:\n            frac_pos, mean_pred = calibration_curve(y_true, proba, n_bins=10)\n            ax2.plot(mean_pred, frac_pos, \"s-\", color=PALETTE[\"holdout\"], label=\"Model\")\n            ax2.plot([0, 1], [0, 1], \"k--\", label=\"Perfect\")\n        except Exception:\n            ax2.text(0.5, 0.5, \"Calibration N/A\", ha=\"center\", transform=ax2.transAxes)\n        ax2.set_title(\"Calibration Curve (Holdout)\")\n        ax2.set_xlabel(\"Mean Predicted Probability\")\n        ax2.set_ylabel(\"Fraction of Positives\")\n        ax2.legend()\n\n        # 3. ROC\n        ax3 = fig.add_subplot(gs[0, 2])\n        fpr, tpr, _ = roc_curve(y_true, proba)\n        ax3.plot(fpr, tpr, color=PALETTE[\"holdout\"], linewidth=2,\n                 label=f\"AUC={m['roc_auc']:.3f}\")\n        ax3.plot([0, 1], [0, 1], \"k--\")\n        ax3.set_title(\"ROC Curve (Holdout)\")\n        ax3.set_xlabel(\"FPR\"); ax3.set_ylabel(\"TPR\")\n        ax3.legend()\n\n        # 4. Confusion matrix\n        ax4 = fig.add_subplot(gs[1, 0])\n        cm = confusion_matrix(y_true, pred)\n        sns.heatmap(cm, annot=True, fmt=\"d\", ax=ax4, cmap=\"RdYlGn\",\n                    xticklabels=[\"Low\", \"High\"],\n                    yticklabels=[\"Low\", \"High\"])\n        ax4.set_title(f\"Confusion Matrix (Holdout)\\nacc={m['accuracy']:.3f}\")\n        ax4.set_xlabel(\"Predicted\"); ax4.set_ylabel(\"Actual\")\n\n        # 5. Error analysis — hard samples\n        ax5 = fig.add_subplot(gs[1, 1])\n        errors = np.abs(y_true - proba)\n        ax5.hist(errors, bins=40, color=\"#FF7043\", edgecolor=\"white\")\n        ax5.set_title(\"Error Magnitude Distribution (Holdout)\")\n        ax5.set_xlabel(\"|True − Predicted Probability|\")\n        ax5.set_ylabel(\"Count\")\n\n        # 6. Metric summary text\n        ax6 = fig.add_subplot(gs[1, 2])\n        ax6.axis(\"off\")\n        report_txt = (\n            f\"HOLDOUT EVALUATION REPORT\\n\"\n            f\"{'─' * 30}\\n\"\n            f\"Samples:      {m['n_samples']}\\n\"\n            f\"Accuracy:     {m['accuracy']:.4f}\\n\"\n            f\"Precision:    {m['precision']:.4f}\\n\"\n            f\"Recall:       {m['recall']:.4f}\\n\"\n            f\"F1 Score:     {m['f1']:.4f}\\n\"\n            f\"ROC-AUC:      {m['roc_auc']:.4f}\\n\"\n            f\"Avg Precision:{m['avg_precision']:.4f}\\n\"\n        )\n        ax6.text(0.05, 0.95, report_txt, transform=ax6.transAxes,\n                 fontsize=11, verticalalignment=\"top\",\n                 fontfamily=\"monospace\",\n                 bbox=dict(boxstyle=\"round\", facecolor=\"lightyellow\", alpha=0.8))\n\n        fig.suptitle(\"Holdout (Unseen Data) — Deep Dive Analysis\",\n                     fontsize=14, fontweight=\"bold\")\n        plt.tight_layout()\n        self.save_show(PLOTS_DIR / \"eval_holdout_deep_dive.png\")\n\n    def plot_model_comparison(self, base_results: dict) -> None:\n        \"\"\"Compare all classical models on val accuracy + AUC.\"\"\"\n        names  = list(base_results.keys())\n        accs   = [base_results[n][\"val_acc\"] for n in names]\n        aucs   = [base_results[n][\"val_auc\"] for n in names]\n        times  = [base_results[n][\"time_s\"] for n in names]\n\n        x = np.arange(len(names))\n        width = 0.35\n\n        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n        axes[0].bar(x - width/2, accs, width, label=\"Accuracy\", color=\"#42A5F5\")\n        axes[0].bar(x + width/2, aucs, width, label=\"ROC-AUC\",  color=\"#66BB6A\")\n        axes[0].set_xticks(x); axes[0].set_xticklabels(names, rotation=15, ha=\"right\")\n        axes[0].set_ylim(0, 1.1)\n        axes[0].set_title(\"Model Comparison — Val Accuracy & AUC\",\n                           fontsize=12, fontweight=\"bold\")\n        axes[0].legend()\n        axes[0].axhline(0.9, color=\"red\", linestyle=\"--\", linewidth=1)\n        axes[0].grid(axis=\"y\", alpha=0.3)\n        for i, (a, au) in enumerate(zip(accs, aucs)):\n            axes[0].text(i - width/2, a + 0.01, f\"{a:.3f}\",\n                         ha=\"center\", fontsize=8)\n            axes[0].text(i + width/2, au + 0.01, f\"{au:.3f}\",\n                         ha=\"center\", fontsize=8)\n\n        axes[1].bar(names, times, color=\"#EF5350\")\n        axes[1].set_title(\"Training Time (seconds)\", fontsize=12, fontweight=\"bold\")\n        axes[1].set_ylabel(\"Seconds\")\n        plt.xticks(rotation=15, ha=\"right\")\n        axes[1].grid(axis=\"y\", alpha=0.3)\n\n        plt.tight_layout()\n        self.save_show(PLOTS_DIR / \"eval_model_comparison.png\")\n\n    def plot_tsne(self, X: np.ndarray, y: np.ndarray, split_name: str) -> None:\n        \"\"\"t-SNE projection of feature space, coloured by class.\"\"\"\n        n = min(800, len(X))\n        idx = np.random.choice(len(X), n, replace=False)\n        X_s, y_s = X[idx], y[idx]\n\n        logger.info(f\"  t-SNE on {split_name} ({n} samples) ...\")\n        try:\n            tsne = TSNE(n_components=2, perplexity=30, max_iter=500,\n                        random_state=RANDOM_STATE)\n            emb = tsne.fit_transform(X_s)\n\n            fig, ax = plt.subplots(figsize=(8, 7))\n            for cls, color, label in [(0, PALETTE[\"neg\"], \"Low Risk\"),\n                                       (1, PALETTE[\"pos\"], \"High Risk\")]:\n                mask = y_s == cls\n                ax.scatter(emb[mask, 0], emb[mask, 1],\n                           c=color, alpha=0.6, s=20, label=label)\n            ax.set_title(f\"t-SNE Feature Space ({split_name})\",\n                         fontsize=12, fontweight=\"bold\")\n            ax.legend()\n            ax.grid(alpha=0.3)\n            plt.tight_layout()\n            self.save_show(PLOTS_DIR / f\"viz_tsne_{split_name}.png\")\n        except Exception as exc:\n            logger.warning(f\"  t-SNE failed: {exc}\")\n        force_cleanup()\n\n    def plot_cv_results(self, cv_summary: dict) -> None:\n        \"\"\"Box-plot style summary of cross-validation results.\"\"\"\n        metrics = list(cv_summary.keys())\n        means   = [cv_summary[m][\"mean\"] for m in metrics]\n        stds    = [cv_summary[m][\"std\"]  for m in metrics]\n\n        fig, ax = plt.subplots(figsize=(8, 5))\n        x = np.arange(len(metrics))\n        bars = ax.bar(x, means, yerr=stds, capsize=6,\n                      color=\"#7E57C2\", alpha=0.85, error_kw=dict(linewidth=2))\n        ax.set_xticks(x); ax.set_xticklabels(metrics, fontsize=11)\n        ax.set_ylim(0, 1.1)\n        ax.set_title(\"5-Fold Cross-Validation Results (mean ± std)\",\n                     fontsize=12, fontweight=\"bold\")\n        ax.set_ylabel(\"Score\")\n        for bar, m, s in zip(bars, means, stds):\n            ax.text(bar.get_x() + bar.get_width()/2,\n                    bar.get_height() + s + 0.01,\n                    f\"{m:.3f}±{s:.3f}\", ha=\"center\", fontsize=9)\n        ax.grid(axis=\"y\", alpha=0.3)\n        plt.tight_layout()\n        self.save_show(PLOTS_DIR / \"eval_cv_results.png\")\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 14 — MODEL SAVING\n# ══════════════════════════════════════════════════════════════════════════════\n\ndef save_best_model(model, model_name: str, metrics: dict,\n                    feature_names: list, scaler, engineer) -> None:\n    \"\"\"Save best model + metadata.\"\"\"\n    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    model_path = MODELS_DIR / f\"best_model_{model_name}_{ts}.pkl\"\n    meta_path  = MODELS_DIR / f\"best_model_meta_{ts}.json\"\n\n    joblib.dump({\"model\": model, \"scaler\": scaler,\n                 \"feature_names\": feature_names,\n                 \"selected_features\": engineer.selected_features,\n                 \"pca\": engineer.pca}, model_path)\n\n    meta = {\n        \"model_name\":  model_name,\n        \"timestamp\":   ts,\n        \"val_accuracy\":    metrics.get(\"val_accuracy\", 0),\n        \"test_accuracy\":   metrics.get(\"test_accuracy\", 0),\n        \"holdout_accuracy\":metrics.get(\"holdout_accuracy\", 0),\n        \"val_auc\":         metrics.get(\"val_auc\", 0),\n        \"features\":        feature_names[:50],\n        \"n_features\":      len(feature_names),\n    }\n    with open(meta_path, \"w\") as f:\n        json.dump(meta, f, indent=2)\n\n    logger.info(f\"  Model saved: {model_path}\")\n    logger.info(f\"  Meta  saved: {meta_path}\")\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  SECTION 15 — FINAL REPORT\n# ══════════════════════════════════════════════════════════════════════════════\n\ndef generate_text_report(all_metrics: dict, cv_summary: dict,\n                          verdict: str, data_source: str,\n                          best_model_name: str) -> None:\n    \"\"\"Write comprehensive text report.\"\"\"\n    ts   = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    path = REPORTS_DIR / \"health_risk_pipeline_report.txt\"\n\n    lines = [\n        \"=\" * 70,\n        \"  HEALTH RISK SIGNAL DETECTION — PIPELINE REPORT\",\n        f\"  Generated: {ts}\",\n        \"=\" * 70,\n        f\"  Data Source: {data_source}\",\n        f\"  Best Model:  {best_model_name}\",\n        \"\",\n        \"SPLIT EVALUATION\",\n        \"-\" * 40,\n    ]\n    for split, m in all_metrics.items():\n        lines += [\n            f\"  {split.upper():8s}  n={m['n_samples']:5d}  \"\n            f\"acc={m['accuracy']:.4f}  auc={m['roc_auc']:.4f}  \"\n            f\"f1={m['f1']:.4f}  prec={m['precision']:.4f}  rec={m['recall']:.4f}\",\n        ]\n\n    lines += [\"\", \"CROSS-VALIDATION (5-fold)\", \"-\" * 40]\n    for k, s in cv_summary.items():\n        lines.append(f\"  {k:12s}: {s['mean']:.4f} ± {s['std']:.4f}\")\n\n    lines += [\"\", \"OVERFITTING VERDICT\", \"-\" * 40, f\"  {verdict}\", \"\"]\n    lines += [\"GENERALISATION GAPS\", \"-\" * 40]\n    train_acc = all_metrics.get(\"train\", {}).get(\"accuracy\", 0)\n    for split in [\"val\", \"test\", \"holdout\"]:\n        gap = train_acc - all_metrics.get(split, {}).get(\"accuracy\", train_acc)\n        lines.append(f\"  Train − {split:7s}: {gap:+.4f}\")\n\n    lines += [\"\", \"PLOTS GENERATED\", \"-\" * 40]\n    for p in sorted(PLOTS_DIR.glob(\"*.png\")):\n        lines.append(f\"  {p.name}\")\n\n    report_text = \"\\n\".join(lines)\n    with open(path, \"w\") as f:\n        f.write(report_text)\n    print(report_text)\n    logger.info(f\"  Report saved: {path}\")\n\n\n# ══════════════════════════════════════════════════════════════════════════════\n#  MAIN PIPELINE ORCHESTRATOR\n# ══════════════════════════════════════════════════════════════════════════════\n\ndef main():\n    print(\"=\" * 70)\n    print(\"  HEALTH RISK SIGNAL DETECTION — AI/ML PIPELINE\")\n    print(\"  Using open-source self-reported longitudinal datasets\")\n    print(\"=\" * 70)\n    log_memory(\"pipeline start\")\n\n    # ── 1. Load Data ───────────────────────────────────────────────────\n    loader = DataLoader()\n    df = loader.load()\n    log_memory(\"after data load\")\n\n    # ── 2. Identify target column ──────────────────────────────────────\n    TARGET_COL = \"health_risk\"\n    # Try to auto-detect a suitable target if not present\n    if TARGET_COL not in df.columns:\n        candidates = [\"target\", \"heart_disease\", \"disease\", \"label\",\n                      \"cardiovascular_disease\", \"outcome\"]\n        for cand in candidates:\n            if cand in df.columns:\n                df = df.rename(columns={cand: TARGET_COL})\n                break\n        if TARGET_COL not in df.columns:\n            # Build from available clinical columns\n            from sklearn.preprocessing import Binarizer\n            num_df = df.select_dtypes(include=np.number)\n            if not num_df.empty:\n                score = StandardScaler().fit_transform(num_df).mean(axis=1)\n                df[TARGET_COL] = (score > np.percentile(score, 65)).astype(int)\n            else:\n                raise RuntimeError(\"Cannot determine target column.\")\n\n    # Binary target check\n    df[TARGET_COL] = (df[TARGET_COL] > 0).astype(np.int8)\n    logger.info(f\"Target: {TARGET_COL}  pos_rate={df[TARGET_COL].mean():.2%}  n={len(df)}\")\n\n    # ── 3. EDA ─────────────────────────────────────────────────────────\n    preproc = Preprocessor(target_col=TARGET_COL)\n    preproc.run_eda(df)\n    force_cleanup()\n\n    # ── 4. Clean & Preprocess ─────────────────────────────────────────\n    df = preproc.clean(df)\n    df = preproc.remove_outliers_iqr(df, preproc.numerical_cols)\n    log_memory(\"after clean\")\n\n    # ── 5. Feature Engineering ────────────────────────────────────────\n    eng = FeatureEngineer()\n    df = eng.engineer(df, target_col=TARGET_COL)\n    df = df.fillna(df.median(numeric_only=True))    # Final NaN sweep\n    log_memory(\"after feature engineering\")\n\n    # ── 6. Separate features / target ─────────────────────────────────\n    feature_cols = [c for c in df.columns if c != TARGET_COL]\n    X_df = df[feature_cols].select_dtypes(include=[np.number])\n    y    = df[TARGET_COL].values.astype(np.int8)\n\n    logger.info(f\"Feature matrix: {X_df.shape}  Target distribution: {np.bincount(y)}\")\n\n    # ── 7. Feature Selection ──────────────────────────────────────────\n    X_df = eng.select_features(X_df, pd.Series(y), n_features=35)\n    feature_names = X_df.columns.tolist()\n    X_raw = X_df.values.astype(np.float32)\n    force_cleanup(df, X_df)\n\n    # ── 8. Four-way split ─────────────────────────────────────────────\n    splits = four_way_split(X_raw, y)\n    X_train, y_train = splits[\"train\"]\n    X_val,   y_val   = splits[\"val\"]\n    X_test,  y_test  = splits[\"test\"]\n    X_hold,  y_hold  = splits[\"holdout\"]\n\n    # ── 9. Scaling (fit on train only) ────────────────────────────────\n    X_train, X_val, X_test, X_hold = preproc.scale(\n        X_train, X_val, X_test, X_hold)\n    log_memory(\"after scaling\")\n\n    # ── 10. PCA augmentation ──────────────────────────────────────────\n    n_pca = min(8, X_train.shape[1] - 1)\n    X_train_pca = eng.compute_pca(X_train, n_components=n_pca)\n    X_val_pca   = np.hstack([X_val,  eng.pca.transform(X_val)])\n    X_test_pca  = np.hstack([X_test, eng.pca.transform(X_test)])\n    X_hold_pca  = np.hstack([X_hold, eng.pca.transform(X_hold)])\n    logger.info(f\"Shape with PCA: {X_train_pca.shape}\")\n\n    # ── 11. Sample weights ────────────────────────────────────────────\n    sample_weights = compute_sample_weights(X_train_pca, y_train)\n    log_memory(\"after sample weights\")\n\n    # ── 12. Train classical models ────────────────────────────────────\n    classic_models = build_classical_models(X_train_pca.shape[1])\n    base_results   = train_classical_models(\n        classic_models, X_train_pca, y_train,\n        X_val_pca,   y_val, sample_weights)\n    log_memory(\"after classical training\")\n\n    # ── 13. Hyperparameter tuning (best classical model) ──────────────\n    best_cls_name = max(base_results, key=lambda n: base_results[n][\"val_acc\"])\n    logger.info(f\"Best classical model: {best_cls_name} \"\n                f\"(val_acc={base_results[best_cls_name]['val_acc']:.4f})\")\n\n    tuned_model = tune_hyperparameters(\n        X_train_pca, y_train, X_val_pca, y_val, best_cls_name)\n    if tuned_model is not None:\n        tuned_val_acc = accuracy_score(y_val, tuned_model.predict(X_val_pca))\n        if tuned_val_acc > base_results[best_cls_name][\"val_acc\"]:\n            base_results[best_cls_name][\"model\"] = tuned_model\n            base_results[best_cls_name][\"val_acc\"] = tuned_val_acc\n            logger.info(f\"  Tuning improved: {tuned_val_acc:.4f}\")\n    log_memory(\"after hyperparameter tuning\")\n\n    # ── 14. Neural Network ────────────────────────────────────────────\n    nn_model, nn_history = train_neural_network(\n        X_train_pca, y_train, X_val_pca, y_val, sample_weights)\n    log_memory(\"after NN training\")\n\n    # ── 15. Stacking Ensemble ─────────────────────────────────────────\n    stacking_result = build_stacking_ensemble(\n        base_results, X_train_pca, y_train, X_val_pca, y_val)\n    if stacking_result is not None:\n        stacking_model, stack_val_acc = stacking_result\n        base_results[\"Stacking\"] = {\n            \"model\":   stacking_model,\n            \"val_acc\": stack_val_acc,\n            \"val_auc\": roc_auc_score(y_val,\n                           stacking_model.predict_proba(X_val_pca)[:, 1]),\n            \"time_s\":  0,\n        }\n    log_memory(\"after ensemble\")\n\n    # ── 16. Select overall best classical model ───────────────────────\n    best_model_name = max(base_results, key=lambda n: base_results[n][\"val_acc\"])\n    best_model      = base_results[best_model_name][\"model\"]\n    logger.info(f\"Final best model: {best_model_name}\")\n\n    # ── 17. Cross-validation ──────────────────────────────────────────\n    X_trainval = np.vstack([X_train_pca, X_val_pca])\n    y_trainval = np.concatenate([y_train, y_val])\n    cv_summary = run_cross_validation(best_model, X_trainval, y_trainval)\n\n    # ── 18. Evaluate on all splits ────────────────────────────────────\n    logger.info(\"=\" * 50)\n    logger.info(\"FINAL EVALUATION — ALL SPLITS\")\n    logger.info(\"=\" * 50)\n    all_metrics = {}\n    split_data  = {\n        \"train\":   (X_train_pca, y_train),\n        \"val\":     (X_val_pca,   y_val),\n        \"test\":    (X_test_pca,  y_test),\n        \"holdout\": (X_hold_pca,  y_hold),\n    }\n    for split_name, (Xs, ys) in tqdm(split_data.items(), desc=\"Evaluating splits\"):\n        m = evaluate_model(best_model, Xs, ys, split_name, nn_model)\n        m[\"y_true\"] = ys\n        all_metrics[split_name] = m\n\n    # ── 19. Overfitting detection ─────────────────────────────────────\n    verdict = detect_overfitting(\n        all_metrics[\"train\"], all_metrics[\"val\"],\n        all_metrics[\"test\"],  all_metrics[\"holdout\"])\n\n    # ── 20. Save best model ───────────────────────────────────────────\n    best_metrics = {\n        \"val_accuracy\":     all_metrics[\"val\"][\"accuracy\"],\n        \"test_accuracy\":    all_metrics[\"test\"][\"accuracy\"],\n        \"holdout_accuracy\": all_metrics[\"holdout\"][\"accuracy\"],\n        \"val_auc\":          all_metrics[\"val\"][\"roc_auc\"],\n    }\n    save_best_model(best_model, best_model_name, best_metrics,\n                    feature_names, preproc.scaler, eng)\n\n    # ── 21. Visualisations ────────────────────────────────────────────\n    viz = Visualiser()\n\n    with tqdm(total=10, desc=\"Generating plots\") as pbar:\n        viz.plot_roc_curves(all_metrics);           pbar.update(1); force_cleanup()\n        viz.plot_precision_recall(all_metrics);     pbar.update(1); force_cleanup()\n        viz.plot_confusion_matrices(all_metrics);   pbar.update(1); force_cleanup()\n        viz.plot_metrics_bar(all_metrics);          pbar.update(1); force_cleanup()\n        viz.plot_generalization_gap(all_metrics);   pbar.update(1); force_cleanup()\n        viz.plot_nn_history(nn_history);            pbar.update(1); force_cleanup()\n        viz.plot_holdout_deep_dive(all_metrics[\"holdout\"]); pbar.update(1); force_cleanup()\n        viz.plot_model_comparison(base_results);    pbar.update(1); force_cleanup()\n        viz.plot_cv_results(cv_summary);            pbar.update(1); force_cleanup()\n        viz.plot_tsne(X_hold_pca, y_hold, \"holdout\"); pbar.update(1); force_cleanup()\n\n    viz.plot_permutation_importance(\n        best_model, X_hold_pca, y_hold,\n        feature_names + [f\"PC{i}\" for i in range(n_pca)])\n\n    # ── 22. Final report ─────────────────────────────────────────────\n    generate_text_report(all_metrics, cv_summary, verdict,\n                         loader.data_source, best_model_name)\n\n    log_memory(\"pipeline end\")\n    logger.info(\"=\" * 60)\n    logger.info(\"PIPELINE COMPLETE\")\n    logger.info(f\"  Holdout accuracy: {all_metrics['holdout']['accuracy']:.4f}\")\n    logger.info(f\"  Holdout AUC:      {all_metrics['holdout']['roc_auc']:.4f}\")\n    logger.info(f\"  Plots:            {PLOTS_DIR}/\")\n    logger.info(f\"  Models:           {MODELS_DIR}/\")\n    logger.info(f\"  Reports:          {REPORTS_DIR}/\")\n    logger.info(\"=\" * 60)\n\n    # Final memory sweep\n    force_cleanup(X_train_pca, X_val_pca, X_test_pca, X_hold_pca,\n                  X_raw, y_train, y_val, y_test, y_hold)\n    gc.collect()\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-13T20:58:00.345328Z","iopub.execute_input":"2026-02-13T20:58:00.345768Z","iopub.status.idle":"2026-02-13T21:00:29.721653Z","shell.execute_reply.started":"2026-02-13T20:58:00.345729Z","shell.execute_reply":"2026-02-13T21:00:29.720556Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771016292.936209      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771016293.014313      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771016293.598820      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771016293.598910      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771016293.598913      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771016293.598917      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2026-02-13 20:58:29,349 [INFO] [MEM pipeline start] Process=1016MB  System=5.1% used  Available=31958MB\n2026-02-13 20:58:29,350 [INFO] ============================================================\n2026-02-13 20:58:29,350 [INFO] DATA LOADING — open-source datasets\n2026-02-13 20:58:29,351 [INFO] ============================================================\n2026-02-13 20:58:29,353 [INFO] [MEM before load] Process=1016MB  System=5.1% used  Available=31958MB\n2026-02-13 20:58:29,354 [INFO]   Attempting: NHANES 2017-2018\n","output_type":"stream"},{"name":"stdout","text":"======================================================================\n  HEALTH RISK SIGNAL DETECTION — AI/ML PIPELINE\n  Using open-source self-reported longitudinal datasets\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"  Downloading NHANES XPT: 100%|██████████| 13/13 [00:03<00:00,  3.57it/s]\n2026-02-13 20:58:32,996 [INFO]   Attempting: UCI Heart Disease\n2026-02-13 20:58:33,602 [INFO]   UCI Heart loaded from https://archive.ics.uci.edu/ml/machine-learning-da\n2026-02-13 20:58:33,603 [INFO]    Loaded UCI Heart Disease: (303, 14)\n2026-02-13 20:58:33,617 [INFO] [MEM after load] Process=1018MB  System=5.1% used  Available=31959MB\n2026-02-13 20:58:33,619 [INFO] [MEM after data load] Process=1018MB  System=5.1% used  Available=31959MB\n2026-02-13 20:58:33,627 [INFO] Target: health_risk  pos_rate=45.87%  n=303\n2026-02-13 20:58:33,628 [INFO] Running EDA ...\n2026-02-13 20:58:33,658 [INFO] Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n2026-02-13 20:58:33,661 [INFO] Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n2026-02-13 20:58:34,048 [INFO]   Saved: plots/eda_class_distribution.png\n2026-02-13 20:58:34,351 [INFO]   Saved: plots/eda_missing_values.png\n2026-02-13 20:58:38,454 [INFO]   Saved: plots/eda_feature_distributions.png\n2026-02-13 20:58:40,205 [INFO]   Saved: plots/eda_correlation_heatmap.png\n2026-02-13 20:58:42,873 [INFO]   Saved: plots/eda_feature_vs_target.png\n2026-02-13 20:58:46,007 [INFO] Cleaning data ...\n  Cleaning steps: 100%|██████████| 7/7 [00:00<00:00, 558.42it/s]\n2026-02-13 20:58:46,023 [INFO]   Shape: (303, 14) → (303, 14)\n2026-02-13 20:58:46,024 [INFO] Outlier detection (IQR) ...\n2026-02-13 20:58:46,197 [INFO]   Saved: plots/eda_outlier_summary.png\n2026-02-13 20:58:46,199 [INFO] [MEM after clean] Process=1098MB  System=5.3% used  Available=31872MB\n2026-02-13 20:58:46,199 [INFO] Feature Engineering ...\n  Engineering features: 100%|██████████| 8/8 [00:00<00:00, 2555.94it/s]\n2026-02-13 20:58:46,206 [INFO]   Feature count after engineering: 16\n2026-02-13 20:58:46,213 [INFO] [MEM after feature engineering] Process=1098MB  System=5.3% used  Available=31872MB\n2026-02-13 20:58:46,215 [INFO] Feature matrix: (303, 15)  Target distribution: [164 139]\n2026-02-13 20:58:46,216 [INFO]   Selecting top 35 features ...\n2026-02-13 20:58:47,611 [INFO]   Saved: plots/feature_importance.png\n2026-02-13 20:58:48,707 [INFO] Creating four-way stratified split ...\n2026-02-13 20:58:48,712 [INFO] Verifying data splits ...\n2026-02-13 20:58:48,713 [INFO]    train   : n=  120  frac=0.396  expected≈0.40  pos_rate=0.458\n2026-02-13 20:58:48,714 [INFO]    val     : n=   46  frac=0.152  expected≈0.15  pos_rate=0.457\n2026-02-13 20:58:48,715 [INFO]    test    : n=   46  frac=0.152  expected≈0.15  pos_rate=0.457\n2026-02-13 20:58:48,715 [INFO]    holdout : n=   91  frac=0.300  expected≈0.30  pos_rate=0.462\n2026-02-13 20:58:48,721 [INFO] [MEM after scaling] Process=1100MB  System=5.3% used  Available=31877MB\n2026-02-13 20:58:49,198 [INFO]   Saved: plots/feature_pca_variance.png\n2026-02-13 20:58:49,202 [INFO] Shape with PCA: (120, 23)\n2026-02-13 20:58:49,203 [INFO] Computing outlier-adjusted sample weights ...\n2026-02-13 20:58:50,712 [INFO]   Weight range: [0.300, 1.000]\n2026-02-13 20:58:50,715 [INFO] [MEM after sample weights] Process=1057MB  System=5.2% used  Available=31903MB\nTraining classical models:   0%|          | 0/5 [00:00<?, ?it/s]2026-02-13 20:58:50,766 [INFO]   GradientBoosting      val_acc=0.8696  val_auc=0.8895  t=0.0s\nTraining classical models:  20%|██        | 1/5 [00:00<00:01,  2.07it/s]2026-02-13 20:58:51,681 [INFO]   RandomForest          val_acc=0.8261  val_auc=0.9333  t=0.4s\nTraining classical models:  40%|████      | 2/5 [00:01<00:02,  1.45it/s]2026-02-13 20:58:52,047 [INFO]   LogisticRegression    val_acc=0.8261  val_auc=0.9200  t=0.0s\nTraining classical models:  60%|██████    | 3/5 [00:01<00:01,  1.85it/s]2026-02-13 20:58:52,525 [INFO]   XGBoost               val_acc=0.7826  val_auc=0.9029  t=0.1s\nTraining classical models:  80%|████████  | 4/5 [00:02<00:00,  1.84it/s]2026-02-13 20:58:53,034 [INFO]   LightGBM              val_acc=0.8261  val_auc=0.9029  t=0.1s\nTraining classical models: 100%|██████████| 5/5 [00:02<00:00,  1.85it/s]\n2026-02-13 20:58:53,416 [INFO] [MEM after classical training] Process=1067MB  System=5.2% used  Available=31914MB\n2026-02-13 20:58:53,417 [INFO] Best classical model: GradientBoosting (val_acc=0.8696)\n2026-02-13 20:58:53,418 [INFO] Hyperparameter tuning for GradientBoosting ...\n  Hyperparameter grid: 100%|██████████| 36/36 [00:53<00:00,  1.48s/it]\n2026-02-13 20:59:46,829 [INFO]   Best params: {'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 300, 'subsample': 0.9}  val_acc=0.8478\n2026-02-13 20:59:46,833 [INFO] [MEM after hyperparameter tuning] Process=1067MB  System=5.1% used  Available=31951MB\n2026-02-13 20:59:46,834 [INFO] Training AttentionNet ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"name":"stderr","text":"2026-02-13 20:59:47,044 [INFO]   Model: \"AttentionNet\"\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input (InputLayer)  │ (None, 23)        │          0 │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ gaussian_noise      │ (None, 23)        │          0 │ input[0][0]       │\n│ (GaussianNoise)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (Dense)       │ (None, 128)       │      3,072 │ gaussian_noise[0… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalization │ (None, 128)       │        512 │ dense[0][0]       │\n│ (BatchNormalizatio… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (Dropout)   │ (None, 128)       │          0 │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (Dense)     │ (None, 64)        │      8,256 │ dropout[0][0]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 64)        │        256 │ dense_1[0][0]     │\n│ (BatchNormalizatio… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_1 (Dropout) │ (None, 64)        │          0 │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ reshape (Reshape)   │ (None, 1, 64)     │          0 │ dropout_1[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ [(None, 1, 64),   │     16,640 │ reshape[0][0],    │\n│ (MultiHeadAttentio… │ (None, 4, 1, 1)]  │            │ reshape[0][0]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add (Add)           │ (None, 1, 64)     │          0 │ multi_head_atten… │\n│                     │                   │            │ reshape[0][0]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalization │ (None, 1, 64)     │        128 │ add[0][0]         │\n│ (LayerNormalizatio… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (None, 64)        │          0 │ layer_normalizat… │\n│ (GlobalAveragePool… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (Dense)     │ (None, 32)        │      2,080 │ global_average_p… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 32)        │        128 │ dense_2[0][0]     │\n│ (BatchNormalizatio… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_3 (Dropout) │ (None, 32)        │          0 │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ output (Dense)      │ (None, 1)         │         33 │ dropout_3[0][0]   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n Total params: 31,105 (121.50 KB)\n Trainable params: 30,657 (119.75 KB)\n Non-trainable params: 448 (1.75 KB)\n\n2026-02-13 20:59:56,277 [INFO]   Best val_accuracy: 0.7826\n2026-02-13 20:59:56,278 [INFO] [MEM after NN training] Process=1158MB  System=5.3% used  Available=31885MB\n2026-02-13 20:59:56,279 [INFO] Building stacking ensemble ...\n2026-02-13 21:00:00,763 [INFO]   Stacking val_acc=0.8696\n2026-02-13 21:00:01,171 [INFO] [MEM after ensemble] Process=1161MB  System=6.6% used  Available=31439MB\n2026-02-13 21:00:01,172 [INFO] Final best model: GradientBoosting\n2026-02-13 21:00:01,172 [INFO] 5-fold cross-validation ...\n  CV folds: 100%|██████████| 5/5 [00:08<00:00,  1.64s/it]\n2026-02-13 21:00:09,362 [INFO]   CV accuracy: 0.7717 ± 0.0651\n2026-02-13 21:00:09,363 [INFO]   CV roc_auc: 0.8431 ± 0.0579\n2026-02-13 21:00:09,364 [INFO]   CV f1: 0.7456 ± 0.0779\n2026-02-13 21:00:09,364 [INFO] ==================================================\n2026-02-13 21:00:09,365 [INFO] FINAL EVALUATION — ALL SPLITS\n2026-02-13 21:00:09,366 [INFO] ==================================================\nEvaluating splits:   0%|          | 0/4 [00:00<?, ?it/s]2026-02-13 21:00:09,798 [INFO]   train     acc=0.9000  auc=0.9698  f1=0.8929  prec=0.8772  rec=0.9091\nEvaluating splits:  25%|██▌       | 1/4 [00:00<00:01,  2.32it/s]2026-02-13 21:00:09,894 [INFO]   val       acc=0.8478  auc=0.9238  f1=0.8205  prec=0.8889  rec=0.7619\n2026-02-13 21:00:09,986 [INFO]   test      acc=0.8913  auc=0.9467  f1=0.8837  prec=0.8636  rec=0.9048\nEvaluating splits:  75%|███████▌  | 3/4 [00:00<00:00,  5.52it/s]2026-02-13 21:00:10,078 [INFO]   holdout   acc=0.8352  auc=0.9291  f1=0.8352  prec=0.7755  rec=0.9048\nEvaluating splits: 100%|██████████| 4/4 [00:00<00:00,  5.63it/s]\n2026-02-13 21:00:10,080 [INFO]   Overfitting check:  GOOD GENERALISATION — train/holdout gap within healthy range\n2026-02-13 21:00:10,081 [INFO]     Train-Val gap:     0.0522\n2026-02-13 21:00:10,081 [INFO]     Train-Holdout gap: 0.0648\n2026-02-13 21:00:10,082 [INFO]     Generalisation gap:0.0419\n2026-02-13 21:00:10,088 [INFO]   Model saved: models/best_model_GradientBoosting_20260213_210010.pkl\n2026-02-13 21:00:10,089 [INFO]   Meta  saved: models/best_model_meta_20260213_210010.json\nGenerating plots:   0%|          | 0/10 [00:00<?, ?it/s]2026-02-13 21:00:10,362 [INFO]   Saved: plots/eval_roc_curves.png\nGenerating plots:  10%|█         | 1/10 [00:00<00:02,  3.69it/s]2026-02-13 21:00:11,621 [INFO]   Saved: plots/eval_pr_curves.png\nGenerating plots:  20%|██        | 2/10 [00:01<00:06,  1.17it/s]2026-02-13 21:00:13,688 [INFO]   Saved: plots/eval_confusion_matrices.png\nGenerating plots:  30%|███       | 3/10 [00:03<00:09,  1.41s/it]2026-02-13 21:00:15,068 [INFO]   Saved: plots/eval_metrics_bar.png\nGenerating plots:  40%|████      | 4/10 [00:04<00:08,  1.40s/it]2026-02-13 21:00:16,529 [INFO]   Saved: plots/eval_generalization_gap.png\nGenerating plots:  50%|█████     | 5/10 [00:06<00:07,  1.42s/it]2026-02-13 21:00:18,610 [INFO]   Saved: plots/nn_training_history.png\nGenerating plots:  60%|██████    | 6/10 [00:08<00:06,  1.64s/it]2026-02-13 21:00:21,149 [INFO]   Saved: plots/eval_holdout_deep_dive.png\nGenerating plots:  70%|███████   | 7/10 [00:11<00:05,  1.94s/it]2026-02-13 21:00:22,676 [INFO]   Saved: plots/eval_model_comparison.png\nGenerating plots:  80%|████████  | 8/10 [00:12<00:03,  1.81s/it]2026-02-13 21:00:24,150 [INFO]   Saved: plots/eval_cv_results.png\nGenerating plots:  90%|█████████ | 9/10 [00:14<00:01,  1.70s/it]2026-02-13 21:00:25,182 [INFO]   t-SNE on holdout (91 samples) ...\n2026-02-13 21:00:25,711 [INFO]   Saved: plots/viz_tsne_holdout.png\nGenerating plots: 100%|██████████| 10/10 [00:17<00:00,  1.77s/it]\n2026-02-13 21:00:27,795 [INFO]   Permutation importance on holdout ...\n2026-02-13 21:00:28,293 [INFO]   Saved: plots/eval_perm_importance_holdout.png\n2026-02-13 21:00:28,295 [INFO]   Report saved: reports/health_risk_pipeline_report.txt\n2026-02-13 21:00:28,296 [INFO] [MEM pipeline end] Process=1182MB  System=6.7% used  Available=31402MB\n2026-02-13 21:00:28,297 [INFO] ============================================================\n2026-02-13 21:00:28,297 [INFO] PIPELINE COMPLETE\n2026-02-13 21:00:28,298 [INFO]   Holdout accuracy: 0.8352\n2026-02-13 21:00:28,299 [INFO]   Holdout AUC:      0.9291\n2026-02-13 21:00:28,299 [INFO]   Plots:            plots/\n2026-02-13 21:00:28,300 [INFO]   Models:           models/\n2026-02-13 21:00:28,301 [INFO]   Reports:          reports/\n2026-02-13 21:00:28,302 [INFO] ============================================================\n","output_type":"stream"},{"name":"stdout","text":"======================================================================\n  HEALTH RISK SIGNAL DETECTION — PIPELINE REPORT\n  Generated: 2026-02-13 21:00:28\n======================================================================\n  Data Source: UCI Heart Disease\n  Best Model:  GradientBoosting\n\nSPLIT EVALUATION\n----------------------------------------\n  TRAIN     n=  120  acc=0.9000  auc=0.9698  f1=0.8929  prec=0.8772  rec=0.9091\n  VAL       n=   46  acc=0.8478  auc=0.9238  f1=0.8205  prec=0.8889  rec=0.7619\n  TEST      n=   46  acc=0.8913  auc=0.9467  f1=0.8837  prec=0.8636  rec=0.9048\n  HOLDOUT   n=   91  acc=0.8352  auc=0.9291  f1=0.8352  prec=0.7755  rec=0.9048\n\nCROSS-VALIDATION (5-fold)\n----------------------------------------\n  accuracy    : 0.7717 ± 0.0651\n  roc_auc     : 0.8431 ± 0.0579\n  f1          : 0.7456 ± 0.0779\n\nOVERFITTING VERDICT\n----------------------------------------\n   GOOD GENERALISATION — train/holdout gap within healthy range\n\nGENERALISATION GAPS\n----------------------------------------\n  Train − val    : +0.0522\n  Train − test   : +0.0087\n  Train − holdout: +0.0648\n\nPLOTS GENERATED\n----------------------------------------\n  eda_class_distribution.png\n  eda_correlation_heatmap.png\n  eda_feature_distributions.png\n  eda_feature_vs_target.png\n  eda_missing_values.png\n  eda_outlier_summary.png\n  eval_confusion_matrices.png\n  eval_cv_results.png\n  eval_generalization_gap.png\n  eval_holdout_deep_dive.png\n  eval_metrics_bar.png\n  eval_model_comparison.png\n  eval_perm_importance_holdout.png\n  eval_pr_curves.png\n  eval_roc_curves.png\n  feature_importance.png\n  feature_pca_variance.png\n  nn_training_history.png\n  viz_tsne_holdout.png\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}